#!/usr/bin/env python3
"""ç²¾ç®€ç‰ˆ RouteNet ä¸²è¡Œè®­ç»ƒè„šæœ¬ (ä»…æŽ§åˆ¶å°è¾“å‡º)"""

import sys
import time
import subprocess
import argparse
import re
from pathlib import Path
from datetime import datetime

class ModelTrainer:
    def __init__(self, base_dir: str = './', force_retrain: bool = False,
                 enable_early_stopping: bool = True, early_stopping_patience: int = 5,
                 dropout_rates=None, lambda_list=None):
        self.base_dir = Path(base_dir)
        self.train_script = self.base_dir / 'routenet' / 'routenet_tf2.py'
        self.train_data_dir = self.base_dir / 'data' / 'routenet' / 'nsfnetbw' / 'tfrecords' / 'train'
        self.eval_data_dir = self.base_dir / 'data' / 'routenet' / 'nsfnetbw' / 'tfrecords' / 'evaluate'
        self.models_base_dir = self.base_dir / 'fixed_model/0928'
        self.force_retrain = force_retrain
        self.enable_early_stopping = enable_early_stopping
        self.early_stopping_patience = early_stopping_patience
        self.dropout_rates = dropout_rates if dropout_rates is not None else [0.1, 0.0]
        self.lambda_list = lambda_list if lambda_list is not None else [0.05]
        self.add_lambda_suffix = len(self.lambda_list) > 1
        self.training_configs = self._generate_training_configs()

    def _generate_training_configs(self):
        configs = []
        model_configs = [
            # # ä¸ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½®
            {"type": "mlp", "use_kan": False, "physics": "none", "use_physics_loss": False, "use_hard_constraint": False},
            {"type": "kan", "use_kan": True, "physics": "none", "use_physics_loss": False, "use_hard_constraint": False},
            # # ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½® - ä¼ ç»Ÿå›ºå®šlambda
            {"type": "mlp", "use_kan": False, "physics": "soft", "use_physics_loss": True, "use_hard_constraint": False},
            {"type": "mlp", "use_kan": False, "physics": "hard", "use_physics_loss": True, "use_hard_constraint": True},
            {"type": "kan", "use_kan": True, "physics": "soft", "use_physics_loss": True, "use_hard_constraint": False},
            {"type": "kan", "use_kan": True, "physics": "hard", "use_physics_loss": True, "use_hard_constraint": True},
            # ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½® - è¯¾ç¨‹å­¦ä¹ 
            # {"type": "mlp", "use_kan": False, "physics": "soft_cl", "use_physics_loss": True, "use_hard_constraint": False, "use_curriculum": True},
            # {"type": "mlp", "use_kan": False, "physics": "hard_cl", "use_physics_loss": True, "use_hard_constraint": True, "use_curriculum": True},
            # {"type": "kan", "use_kan": True, "physics": "soft_cl", "use_physics_loss": True, "use_hard_constraint": False, "use_curriculum": True},
            # {"type": "kan", "use_kan": True, "physics": "hard_cl", "use_physics_loss": True, "use_hard_constraint": True, "use_curriculum": True},
        ]
        for mc in model_configs:
            # é’ˆå¯¹ç‰©ç†çº¦æŸæ¨¡åž‹éåŽ† lambda; éžç‰©ç†æ¨¡åž‹åªäº§ç”Ÿä¸€æ¬¡ (lam=None)
            lam_candidates = self.lambda_list if mc['use_physics_loss'] else [None]
            for lam in lam_candidates:
                for dropout_rate in self.dropout_rates:
                    name_base = f"{mc['type']}_{mc['physics']}"
                    if lam is not None and self.add_lambda_suffix:
                        lam_str = ('{:.4g}'.format(lam)).replace('.', 'p')
                        name_base += f"_l{lam_str}"
                    if dropout_rate == 0.0:
                        name_base += '_nodrop'
                    cfg = {
                        'name': name_base,
                        'model_type': mc['type'],
                        'use_kan': mc['use_kan'],
                        'physics_type': mc['physics'],
                        'use_physics_loss': mc['use_physics_loss'],
                        'use_hard_constraint': mc['use_hard_constraint'],
                        'lambda_physics': lam if lam is not None else 0.0,
                        'use_curriculum': mc.get('use_curriculum', False),
                        'dropout_rate': dropout_rate,
                        'model_dir': self._get_model_dir(mc, lam, nodrop=(dropout_rate==0.0))
                    }
                    configs.append(cfg)
        return configs

    def _get_model_dir(self, mc, lam, nodrop=False):
        base = f"{mc['type']}_{mc['physics']}"
        if mc['use_physics_loss'] and self.add_lambda_suffix and lam is not None:
            lam_str = ('{:.4g}'.format(lam)).replace('.', 'p')
            base += f"_l{lam_str}"
        if nodrop:
            base += '_nodrop'
        return self.models_base_dir / base

    def _build_training_command(self, cfg):
        cmd = [
            'python', str(self.train_script),
            '--train_dir', str(self.train_data_dir),
            '--eval_dir', str(self.eval_data_dir),
            '--model_dir', str(cfg['model_dir']),
            '--target', 'delay',
            '--epochs', '20',
            '--batch_size', '32',
            '--lr_schedule', 'plateau',
            '--learning_rate', '0.001',
            '--plateau_patience', '8',
            '--plateau_factor', '0.5',
            '--seed', '137',
            '--dropout_rate', str(cfg.get('dropout_rate', 0.1))
        ]
        if cfg['use_kan']:
            cmd.append('--kan')
        # ç‰©ç†çº¦æŸå‚æ•°: soft -> physics_loss; hard -> physics_loss + hard_physics
        if cfg['use_physics_loss']:
            cmd.append('--physics_loss')
            if cfg['use_hard_constraint']:
                cmd.append('--hard_physics')
            # æŒ‡å®šlambda
            cmd.extend(['--lambda_physics', str(cfg['lambda_physics'])])
        if self.enable_early_stopping:
            cmd.extend([
                '--early_stopping',
                '--early_stopping_patience', str(self.early_stopping_patience),
                '--early_stopping_min_delta', '1e-6',
                '--early_stopping_restore_best'
            ])
        return cmd

    def train_model(self, cfg, raw_output: bool = False):
        physics_info = cfg['physics_type']
        if cfg['use_physics_loss']:
            physics_info += f" Î»={cfg['lambda_physics']}" + (" (hard)" if cfg['use_hard_constraint'] else " (soft)")
        print(f"\n{'='*70}\nðŸš€ æ¨¡åž‹: {cfg['name']}  (ç±»åž‹: {cfg['model_type'].upper()}  ç‰©ç†: {physics_info}  dropout={cfg.get('dropout_rate',0.1)})")
        model_file = cfg['model_dir'] / ('best_delay_kan_model.weights.h5' if cfg['use_kan'] else 'best_delay_model.weights.h5')
        if model_file.exists() and not self.force_retrain:
            print(f"â­ï¸  å·²å­˜åœ¨ï¼Œè·³è¿‡: {model_file}")
            return True
        cfg['model_dir'].mkdir(parents=True, exist_ok=True)
        cmd = self._build_training_command(cfg)
        print(f"âš¡ å‘½ä»¤: {' '.join(cmd)}")
        print(f"ðŸ• å¼€å§‹: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        start = time.time()
        try:
            p = subprocess.Popen(
                cmd,
                cwd=str(self.base_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            epoch_patterns = [
                r'epoch\s*(\d+)(?:/\d+)?',
                r'epoch\s*:?\s*(\d+)',
                r'(\d+)/\d+.*epoch',
                r'epoch\s+is\s+(\d+)',
            ]
            current_epoch = 0
            total_epochs = 20
            buffer = []
            last_progress_print = 0.0
            last_step_shown = -1
            progress_regex = re.compile(r'Training Epoch (\d+): .*loss=([0-9.]+), step=(\d+)\]')
            eval_progress_regex = re.compile(r'Evaluating Epoch (\d+): .*loss=([0-9.]+)')

            def print_progress(compact_line: str, force=False):
                nonlocal last_progress_print
                now = time.time()
                if force or now - last_progress_print >= 1.0:  # æœ€å¤šæ¯ç§’åˆ·æ–°ä¸€æ¬¡
                    sys.stdout.write('\r' + compact_line + ' '*20)
                    sys.stdout.flush()
                    last_progress_print = now

            while True:
                line = p.stdout.readline()
                if line == '' and p.poll() is not None:
                    break
                if not line:
                    continue
                buffer.append(line)

                if raw_output:
                    # åŽŸå§‹æ¨¡å¼ç›´æŽ¥é€ä¼ 
                    sys.stdout.write(line)
                    sys.stdout.flush()
                    continue

                stripped = line.strip()
                lower = stripped.lower()

                # æ•èŽ·è®­ç»ƒè¿›åº¦è¡Œ (tqdm) å¹¶åŽ‹ç¼©å±•ç¤º
                if stripped.startswith('Training Epoch'):
                    m = progress_regex.search(stripped)
                    if m:
                        ep = int(m.group(1))
                        loss_val = m.group(2)
                        step = int(m.group(3))
                        if ep != current_epoch:
                            current_epoch = ep
                            last_step_shown = -1
                            print()  # æ¢è¡Œï¼Œç»“æŸä¸Šä¸€epochè¿›åº¦
                            print(f"ðŸŸ¢ å¼€å§‹ Epoch {ep}")
                        # ä»…åœ¨æ­¥æ•°å‰è¿›ä¸”æ­¥æ•°ä¸º10çš„å€æ•°æˆ–æœ€ç»ˆåˆ·æ–°èŠ‚æµæ—¶é—´è¶…å‡ºæ—¶æ›´æ–°
                        if step != last_step_shown and (step % 10 == 0):
                            print_progress(f"Epoch {ep} Step {step} loss={loss_val}")
                            last_step_shown = step
                    continue  # ä¸ç›´æŽ¥è¾“å‡ºåŽŸå§‹è¡Œ

                # æ•èŽ·è¯„ä¼°è¿›åº¦
                if stripped.startswith('Evaluating Epoch'):
                    m2 = eval_progress_regex.search(stripped)
                    if m2:
                        ep_eval = m2.group(1)
                        loss_eval = m2.group(2)
                        print_progress(f"[Eval] Epoch {ep_eval} loss={loss_eval}")
                    continue

                # Epoch ç»“æŸ/ç»“æžœè¡Œ - æ­£å¸¸æ‰“å°æ¢è¡Œ
                if 'finished' in lower and 'epoch' in lower:
                    print()  # ç¡®ä¿è¿›åº¦è¡Œæ¢è¡Œ
                    print(stripped)
                    continue
                if 'evaluation loss improved' in lower or 'evaluation loss did not improve' in lower:
                    print(stripped)
                    continue
                if 'saving model' in lower or 'best validation loss' in lower:
                    print(stripped)
                    continue
                if stripped.startswith('ðŸ“š Curriculum Learning') or 'Curriculum Learning' in stripped:
                    print(stripped)
                    continue
                # å…¶ä»–éžåˆ·å±è¾“å‡ºï¼ˆä¿æŒï¼‰
                if stripped:
                    print(stripped)
            p.wait()
            dur = time.time() - start
            if not raw_output:
                print()  # ç»“æŸæœ€åŽä¸€è¡Œçš„\rè¦†ç›–
            if p.returncode == 0:
                print(f"âœ… å®Œæˆ: {cfg['name']}  ç”¨æ—¶ {dur/60:.1f} åˆ†é’Ÿ")
                # è®°å½•å‘½ä»¤ä¸Žè¿è¡Œæ—¶é—´åˆ°æ¨¡åž‹ç›®å½•æ—¥å¿—æ–‡ä»¶ï¼ˆä»…éœ€æ±‚å­—æ®µï¼‰
                try:
                    log_file = cfg['model_dir'] / 'train_run.log'
                    with open(log_file, 'a', encoding='utf-8') as lf:
                        lf.write(
                            f"{datetime.now().isoformat()}\tsecs={dur:.2f}\tcmd={' '.join(cmd)}\n"
                        )
                except Exception as log_e:
                    print(f"âš ï¸ å†™å…¥æ—¥å¿—å¤±è´¥: {log_e}")
                return True
            else:
                print(f"âŒ å¤±è´¥: {cfg['name']}  code={p.returncode}")
                print(''.join(buffer[-40:]))
                return False
        except KeyboardInterrupt:
            print(f"ðŸ›‘ ä¸­æ–­: {cfg['name']}")
            if 'p' in locals():
                p.terminate(); p.wait()
            return False
        except Exception as e:
            print(f"ðŸ’¥ å¼‚å¸¸: {cfg['name']} -> {e}")
            return False

    def train_all_models(self, start_from=None, models_to_train=None, raw_output: bool = False):
        print(f"ðŸŽ¯ æ€»æ¨¡åž‹æ•°: {len(self.training_configs)}")
        for i, c in enumerate(self.training_configs, 1):
            print(f"  {i:2d}. {c['name']}")
        cfgs = self.training_configs
        if models_to_train:
            cfgs = [c for c in cfgs if c['name'] in models_to_train]
            print(f"ðŸŽ¯ è¿‡æ»¤åŽ: {len(cfgs)} -> {models_to_train}")
        if start_from:
            try:
                idx = next(i for i, c in enumerate(cfgs) if c['name'] == start_from)
                cfgs = cfgs[idx:]
                print(f"ðŸ“ ä»Ž {start_from} å¼€å§‹")
            except StopIteration:
                print(f"âš ï¸ æœªæ‰¾åˆ° {start_from}, ä»Žå¤´å¼€å§‹")
        if not cfgs:
            print("âš ï¸ æ— å¯è®­ç»ƒæ¨¡åž‹")
            return
        success = fail = 0
        for i, c in enumerate(cfgs, 1):
            print(f"\nðŸ”¢ è¿›åº¦ {i}/{len(cfgs)} -> {c['name']}")
            ok = self.train_model(c, raw_output=raw_output)
            success += int(ok)
            fail += int(not ok)
            if i < len(cfgs):
                time.sleep(2)
        print(f"\n{'='*60}\nðŸ“Š è®­ç»ƒå®Œæˆ  æˆåŠŸ:{success}  å¤±è´¥:{fail}  æ€»è®¡:{success+fail}\n{'='*60}")

    def list_configs(self):
        print("ðŸ“‹ é…ç½®åˆ—è¡¨:")
        for i, c in enumerate(self.training_configs, 1):
            print(f"  {i:2d}. {c['name']}  type={c['model_type']}  physics={c['physics_type']}")

def main():
    parser = argparse.ArgumentParser(description='RouteNet ä¸²è¡Œè®­ç»ƒè„šæœ¬ (ç²¾ç®€ç‰ˆ)')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæ‰€æœ‰é…ç½®')
    parser.add_argument('--start-from', type=str, help='ä»ŽæŒ‡å®šæ¨¡åž‹åç§°å¼€å§‹è®­ç»ƒ')
    parser.add_argument('--models', nargs='+', help='åªè®­ç»ƒè¿™äº›æ¨¡åž‹')
    parser.add_argument('--base-dir', default='./', help='é¡¹ç›®æ ¹ç›®å½•')
    parser.add_argument('--force', action='store_true', help='å·²å­˜åœ¨æ—¶é‡æ–°è®­ç»ƒ')
    parser.add_argument('--yes', '-y', action='store_true', help='æ— éœ€ç¡®è®¤ç›´æŽ¥å¼€å§‹')
    parser.add_argument('--no-early-stopping', action='store_true', help='ç¦ç”¨æ—©åœ')
    parser.add_argument('--early-stopping-patience', type=int, default=5, help='æ—©åœè€å¿ƒ')
    parser.add_argument('--raw-output', action='store_true', help='æ˜¾ç¤ºåŽŸå§‹å­è¿›ç¨‹è¾“å‡º(å¯èƒ½åˆ·å±)')
    parser.add_argument('--dropouts', nargs='+', type=float, default=[0.1, 0.0], help='éåŽ†çš„dropoutåˆ—è¡¨ï¼Œä¾‹å¦‚: --dropouts 0 0.1')
    parser.add_argument('--lambdas', nargs='+', type=float, default=[0.05], help='éåŽ†çš„lambda_physicsåˆ—è¡¨(ä»…ç‰©ç†çº¦æŸæ¨¡åž‹)ï¼Œä¾‹å¦‚: --lambdas 0.01 0.05')
    args = parser.parse_args()

    trainer = ModelTrainer(
        base_dir=args.base_dir,
        force_retrain=args.force,
        enable_early_stopping=not args.no_early_stopping,
        early_stopping_patience=args.early_stopping_patience,
        dropout_rates=args.dropouts,
        lambda_list=args.lambdas
    )

    if args.list:
        trainer.list_configs(); return
    if not args.models and not args.start_from and not args.yes:
        ans = input(f"ç¡®è®¤è®­ç»ƒå…¨éƒ¨ {len(trainer.training_configs)} ä¸ªæ¨¡åž‹? (y/N): ")
        if ans.lower() != 'y':
            print('å–æ¶ˆ'); return
    elif args.yes:
        print(f"âœ… è‡ªåŠ¨ç¡®è®¤ï¼Œå¼€å§‹è®­ç»ƒ {len(trainer.training_configs)} ä¸ªæ¨¡åž‹")
    trainer.train_all_models(start_from=args.start_from, models_to_train=args.models, raw_output=args.raw_output)

if __name__ == '__main__':
    main()
