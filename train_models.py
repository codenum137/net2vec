#!/usr/bin/env python3
"""
RouteNetæ¨¡å‹ä¸²è¡Œè®­ç»ƒè„šæœ¬
æ”¯æŒè½¯ç¡¬ç‰©ç†é™åˆ¶ã€MLP/KANã€ä¸åŒlambda_physicså‚æ•°çš„ç»„åˆè®­ç»ƒ
"""

import os
import sys
import subprocess
import time
import json
from datetime import datetime
from pathlib import Path
import argparse

class ModelTrainer:
    def __init__(self, base_dir="./", force_retrain=False, enable_early_stopping=True, early_stopping_patience=5):
        self.base_dir = Path(base_dir)
        self.train_script = self.base_dir / "routenet" / "routenet_tf2.py"
        self.train_data_dir = self.base_dir / "data" / "routenet" / "nsfnetbw" / "tfrecords" / "train"
        self.eval_data_dir = self.base_dir / "data" / "routenet" / "nsfnetbw" / "tfrecords" / "evaluate"
        self.models_base_dir = self.base_dir / "fixed_model/0917-opt"
        self.force_retrain = force_retrain  # æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒå·²å­˜åœ¨çš„æ¨¡å‹
        self.enable_early_stopping = enable_early_stopping  # æ˜¯å¦å¯ç”¨æ—©åœæœºåˆ¶
        self.early_stopping_patience = early_stopping_patience  # æ—©åœè€å¿ƒå€¼
        
        # è®­ç»ƒé…ç½®
        self.training_configs = self._generate_training_configs()
        
    def _generate_training_configs(self):
        """ç”Ÿæˆæ‰€æœ‰è®­ç»ƒé…ç½®ç»„åˆ"""
        configs = []
        
        # æ¨¡å‹ç±»å‹å’Œç‰©ç†é™åˆ¶ç»„åˆ
        model_configs = [
            # # ä¸ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½®
            # {"type": "mlp", "use_kan": False, "physics": "none", "use_physics_loss": False, "use_hard_constraint": False},
            # {"type": "kan", "use_kan": True, "physics": "none", "use_physics_loss": False, "use_hard_constraint": False},
            # # ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½® - ä¼ ç»Ÿå›ºå®šlambda
            # {"type": "mlp", "use_kan": False, "physics": "soft", "use_physics_loss": True, "use_hard_constraint": False},
            # {"type": "mlp", "use_kan": False, "physics": "hard", "use_physics_loss": True, "use_hard_constraint": True},
            # {"type": "kan", "use_kan": True, "physics": "soft", "use_physics_loss": True, "use_hard_constraint": False},
            # {"type": "kan", "use_kan": True, "physics": "hard", "use_physics_loss": True, "use_hard_constraint": True},
            # ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½® - è¯¾ç¨‹å­¦ä¹ 
            # {"type": "mlp", "use_kan": False, "physics": "soft_cl", "use_physics_loss": True, "use_hard_constraint": False, "use_curriculum": True},
            # {"type": "mlp", "use_kan": False, "physics": "hard_cl", "use_physics_loss": True, "use_hard_constraint": True, "use_curriculum": True},
            {"type": "kan", "use_kan": True, "physics": "soft_cl", "use_physics_loss": True, "use_hard_constraint": False, "use_curriculum": True},
            {"type": "kan", "use_kan": True, "physics": "hard_cl", "use_physics_loss": True, "use_hard_constraint": True, "use_curriculum": True},
        ]
        
        # lambda_physicså‚æ•°
        lambda_values = [0.1]
        
        for model_config in model_configs:
            if model_config["use_physics_loss"]:
                # ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½®ï¼šç”Ÿæˆå¤šä¸ªlambdaå€¼
                for lambda_val in lambda_values:
                    # è¯¾ç¨‹å­¦ä¹ é…ç½®
                    if model_config.get("use_curriculum", False):
                        config = {
                            "name": f"{model_config['type']}_{model_config['physics']}_{lambda_val}",
                            "model_type": model_config["type"],
                            "use_kan": model_config["use_kan"],
                            "physics_type": model_config["physics"],
                            "use_physics_loss": model_config["use_physics_loss"],
                            "use_hard_constraint": model_config["use_hard_constraint"],
                            "lambda_physics": lambda_val,  # è¿™å°†ä½œä¸ºmax_lambdaä½¿ç”¨
                            "use_curriculum": True,
                            "warmup_epochs": 5,  # çƒ­èº«æœŸé»˜è®¤5è½®
                            "ramp_epochs": 10,   # å¢é•¿æœŸé»˜è®¤10è½®
                            "max_lambda": lambda_val,  # æœ€å¤§lambdaå€¼
                            "model_dir": self._get_model_dir(model_config, lambda_val),
                        }
                        configs.append(config)
                    else:
                        # ä¼ ç»Ÿå›ºå®šlambdaé…ç½®
                        config = {
                            "name": f"{model_config['type']}_{model_config['physics']}_{lambda_val}",
                            "model_type": model_config["type"],
                            "use_kan": model_config["use_kan"],
                            "physics_type": model_config["physics"],
                            "use_physics_loss": model_config["use_physics_loss"],
                            "use_hard_constraint": model_config["use_hard_constraint"],
                            "lambda_physics": lambda_val,
                            "use_curriculum": False,
                            "model_dir": self._get_model_dir(model_config, lambda_val),
                        }
                        configs.append(config)
            else:
                # ä¸ä½¿ç”¨ç‰©ç†çº¦æŸçš„é…ç½®ï¼šåªç”Ÿæˆä¸€ä¸ªé…ç½®
                config = {
                    "name": f"{model_config['type']}_{model_config['physics']}",
                    "model_type": model_config["type"],
                    "use_kan": model_config["use_kan"],
                    "physics_type": model_config["physics"],
                    "use_physics_loss": model_config["use_physics_loss"],
                    "use_hard_constraint": model_config["use_hard_constraint"],
                    "lambda_physics": 0.0,  # ä¸ä½¿ç”¨ç‰©ç†çº¦æŸæ—¶lambdaå€¼æ— æ„ä¹‰
                    "use_curriculum": False,
                    "model_dir": self._get_model_dir(model_config, None),
                }
                configs.append(config)
        
        return configs
    
    def _get_model_dir(self, model_config, lambda_val):
        """ç”Ÿæˆæ¨¡å‹ä¿å­˜ç›®å½• - ä¼˜åŒ–åçš„ç®€æ´ç»“æ„"""
        # ä½¿ç”¨ fixed_model ä½œä¸ºæ ¹ç›®å½•
        if lambda_val is None:
            # ä¸ä½¿ç”¨ç‰©ç†çº¦æŸçš„æƒ…å†µ
            model_dir = self.models_base_dir / f"{model_config['type']}_{model_config['physics']}"
        else:
            # ä½¿ç”¨ç‰©ç†çº¦æŸçš„æƒ…å†µ
            model_dir = self.models_base_dir / f"{model_config['type']}_{model_config['physics']}_{lambda_val}"
        return model_dir
    
    def _build_training_command(self, config):
        """æ„å»ºè®­ç»ƒå‘½ä»¤"""
        cmd = [
            "python", str(self.train_script),
            "--train_dir", str(self.train_data_dir),
            "--eval_dir", str(self.eval_data_dir),
            "--model_dir", str(config["model_dir"]),
            "--target", "delay",
            "--epochs", "20",  # å¢åŠ è®­ç»ƒè½®æ•°ä»¥è·å¾—æ›´å¥½æ•ˆæœ
            "--batch_size", "32",
            "--lr_schedule", "plateau",
            "--learning_rate", "0.001",
            "--plateau_patience", "8",  # å¢åŠ è€å¿ƒå€¼
            "--plateau_factor", "0.5",

        ]
        
        # æ·»åŠ ç‰©ç†æŸå¤±ç›¸å…³å‚æ•°ï¼ˆä»…åœ¨ä½¿ç”¨ç‰©ç†çº¦æŸæ—¶ï¼‰
        if config["use_physics_loss"]:
            cmd.extend(["--physics_loss"])
            
            # è¯¾ç¨‹å­¦ä¹ å‚æ•°
            if config.get("use_curriculum", False):
                cmd.extend([
                    "--curriculum",
                    "--warmup_epochs", str(config.get("warmup_epochs", 5)),
                    "--ramp_epochs", str(config.get("ramp_epochs", 10)),
                    "--max_lambda", str(config.get("max_lambda", config["lambda_physics"]))
                ])
            else:
                # ä¼ ç»Ÿå›ºå®šlambda
                cmd.extend(["--lambda_physics", str(config["lambda_physics"])])
            
            # æ·»åŠ çº¦æŸç±»å‹å‚æ•°
            if config["use_hard_constraint"]:
                cmd.append("--hard_physics")
            
        # æ·»åŠ KANç›¸å…³å‚æ•°
        if config["use_kan"]:
            cmd.append("--kan")  # ä¿®æ­£å‚æ•°åï¼šä½¿ç”¨ --kan è€Œä¸æ˜¯ --use_kan
        
        # æ·»åŠ æ—©åœæœºåˆ¶å‚æ•°
        if self.enable_early_stopping:
            cmd.extend([
                "--early_stopping",
                "--early_stopping_patience", str(8 if config.get("use_curriculum", False) else self.early_stopping_patience),
                "--early_stopping_min_delta", "1e-6",
                "--early_stopping_restore_best"
            ])
            
        return cmd
    
    def train_model(self, config):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹: {config['name']}")
        print(f"ğŸ“ æ¨¡å‹ç›®å½•: {config['model_dir']}")
        
        # æ„å»ºé…ç½®æè¿°
        config_desc = f"{config['model_type'].upper()}, {config['physics_type']}"
        if config.get("use_curriculum", False):
            config_desc += f", è¯¾ç¨‹å­¦ä¹ (max_Î»={config.get('max_lambda', config['lambda_physics'])})"
        else:
            config_desc += f", Î»={config['lambda_physics']}"
        
        print(f"âš™ï¸  é…ç½®: {config_desc}")
        print(f"{'='*60}")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©æ­£ç¡®çš„æ–‡ä»¶åï¼‰
        if config["use_kan"]:
            model_file = config["model_dir"] / "best_delay_kan_model.weights.h5"
        else:
            model_file = config["model_dir"] / "best_delay_model.weights.h5"
            
        if model_file.exists() and not self.force_retrain:
            print(f"â­ï¸  æ¨¡å‹å·²å­˜åœ¨ï¼Œè·³è¿‡è®­ç»ƒ: {model_file}")
            print(f"ğŸ’¡ å¦‚éœ€é‡æ–°è®­ç»ƒï¼Œè¯·ä½¿ç”¨ --force å‚æ•°")
            print(f"{'='*60}")
            return True
        
        # åˆ›å»ºæ¨¡å‹ç›®å½•
        config["model_dir"].mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„
        log_file = config["model_dir"] / "training.log"
        
        # æ„å»ºè®­ç»ƒå‘½ä»¤
        cmd = self._build_training_command(config)
        
        print(f"âš¡ æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        print(f"ğŸ• å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶: {log_file}")
        
        start_time = time.time()
        
        try:
            # æ‰§è¡Œè®­ç»ƒ - åªæ˜¾ç¤ºepochè¿›åº¦ï¼Œè¯¦ç»†è¾“å‡ºå†™å…¥æ—¥å¿—
            print(f"ğŸ“Š å¼€å§‹è®­ç»ƒï¼Œç›‘æ§è¿›åº¦ä¸­...")
            print(f"{'='*60}")
            
            process = subprocess.Popen(
                cmd,
                cwd=str(self.base_dir),
                stdout=subprocess.PIPE,
                stderr=subprocess.STDOUT,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # ç›‘æ§è¾“å‡ºï¼šæ§åˆ¶å°æ˜¾ç¤ºepochè¿›åº¦ï¼Œæ—¥å¿—æ–‡ä»¶ä¿å­˜è¯¦ç»†è¾“å‡º
            output_lines = []
            current_epoch = 0
            total_epochs = 20  # ä»å‘½ä»¤ä¸­è·å–çš„epochæ•°
            
            # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
            with open(log_file, 'w', encoding='utf-8') as log_f:
                # å†™å…¥æ—¥å¿—å¤´éƒ¨ä¿¡æ¯
                log_f.write(f"{'='*80}\n")
                log_f.write(f"è®­ç»ƒå¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                log_f.write(f"æ¨¡å‹é…ç½®: {config['name']}\n")
                log_f.write(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}\n")
                log_f.write(f"{'='*80}\n\n")
                log_f.flush()
                
                while True:
                    output = process.stdout.readline()
                    if output == '' and process.poll() is not None:
                        break
                    if output:
                        output_lines.append(output)
                        
                        # å†™å…¥æ—¥å¿—æ–‡ä»¶ï¼ˆå®æ—¶ï¼‰
                        log_f.write(output)
                        log_f.flush()
                        
                        # æ£€æŸ¥æ˜¯å¦åŒ…å«epochä¿¡æ¯
                        line_lower = output.lower().strip()
                        if 'epoch' in line_lower:
                            # å°è¯•æå–epochæ•°å­— - æ”¯æŒå¤šç§æ ¼å¼
                            import re
                            # åŒ¹é… "Epoch 1/20", "Epoch: 5", "epoch 10", "Training epoch 3" ç­‰æ ¼å¼
                            epoch_patterns = [
                                r'epoch\s*(\d+)(?:/\d+)?',     # Epoch 1/20 æˆ– Epoch 1
                                r'epoch\s*:?\s*(\d+)',         # Epoch: 5 æˆ– Epoch 5
                                r'(\d+)/\d+.*epoch',           # 1/20 epoch æ ¼å¼
                                r'epoch\s+is\s+(\d+)',         # epoch is 12 æ ¼å¼
                            ]
                            
                            epoch_num = None
                            for pattern in epoch_patterns:
                                epoch_match = re.search(pattern, line_lower)
                                if epoch_match:
                                    epoch_num = int(epoch_match.group(1))
                                    break
                            
                            if epoch_num and epoch_num != current_epoch:
                                current_epoch = epoch_num
                                progress = (current_epoch / total_epochs) * 100
                                progress_msg = f"ğŸ“ˆ è®­ç»ƒè¿›åº¦: Epoch {current_epoch}/{total_epochs} ({progress:.1f}%)"
                                print(progress_msg)
                                
                                # åŒæ—¶å†™å…¥æ—¥å¿—æ–‡ä»¶
                                log_f.write(f"\n[PROGRESS] {progress_msg}\n")
                                log_f.flush()
                
                # å†™å…¥æ—¥å¿—å°¾éƒ¨ä¿¡æ¯
                log_f.write(f"\n{'='*80}\n")
                log_f.write(f"è®­ç»ƒç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            
            process.wait()
            end_time = time.time()
            duration = end_time - start_time
            
            # æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
            if current_epoch > 0:
                final_msg = f"ğŸ“ˆ è®­ç»ƒå®Œæˆ: Epoch {current_epoch}/{total_epochs} (100%)"
                print(final_msg)
                
                # å†™å…¥æœ€ç»ˆçŠ¶æ€åˆ°æ—¥å¿—
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"\n[FINAL] {final_msg}\n")
                    log_f.write(f"è®­ç»ƒè€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)\n")
                    log_f.write(f"{'='*80}\n")
            
            full_output = ''.join(output_lines)
            
            if process.returncode == 0:
                print(f"{'='*60}")
                print(f"âœ… è®­ç»ƒæˆåŠŸå®Œæˆ!")
                print(f"â±ï¸  è®­ç»ƒè€—æ—¶: {duration:.1f}ç§’ ({duration/60:.1f}åˆ†é’Ÿ)")
                print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
                
                # å†™å…¥æˆåŠŸçŠ¶æ€åˆ°æ—¥å¿—
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"\n[SUCCESS] è®­ç»ƒæˆåŠŸå®Œæˆ!\n")
                    log_f.write(f"è¿”å›ç : {process.returncode}\n")
                
                # ä¿å­˜è®­ç»ƒç»“æœ
                self._save_training_result(config, True, duration, full_output, "")
                return True
            else:
                print(f"{'='*60}")
                print(f"âŒ è®­ç»ƒå¤±è´¥!")
                print(f"ğŸ’¬ è¿”å›ç : {process.returncode}")
                print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
                
                # å†™å…¥å¤±è´¥çŠ¶æ€åˆ°æ—¥å¿—
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"\n[ERROR] è®­ç»ƒå¤±è´¥!\n")
                    log_f.write(f"è¿”å›ç : {process.returncode}\n")
                
                # ä¿å­˜è®­ç»ƒç»“æœ
                self._save_training_result(config, False, duration, full_output, f"Process returned {process.returncode}")
                return False
                
        except KeyboardInterrupt:
            print(f"{'='*60}")
            print(f"ğŸ›‘ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
            if 'process' in locals():
                process.terminate()
                process.wait()
            duration = time.time() - start_time
            
            # å†™å…¥ä¸­æ–­çŠ¶æ€åˆ°æ—¥å¿—
            try:
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"\n[INTERRUPTED] è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\n")
                    log_f.write(f"ä¸­æ–­æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    log_f.write(f"å·²è¿è¡Œæ—¶é—´: {duration:.1f}ç§’\n")
            except:
                pass
            
            self._save_training_result(config, False, duration, "", "ç”¨æˆ·ä¸­æ–­")
            return False
        except Exception as e:
            print(f"{'='*60}")
            print(f"ğŸ’¥ è®­ç»ƒå¼‚å¸¸: {e}")
            print(f"ğŸ“ è¯¦ç»†æ—¥å¿—: {log_file}")
            duration = time.time() - start_time
            
            # å†™å…¥å¼‚å¸¸çŠ¶æ€åˆ°æ—¥å¿—
            try:
                with open(log_file, 'a', encoding='utf-8') as log_f:
                    log_f.write(f"\n[EXCEPTION] è®­ç»ƒå¼‚å¸¸: {e}\n")
                    log_f.write(f"å¼‚å¸¸æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                    log_f.write(f"å·²è¿è¡Œæ—¶é—´: {duration:.1f}ç§’\n")
            except:
                pass
            
            self._save_training_result(config, False, duration, "", str(e))
            return False
    
    def _save_training_result(self, config, success, duration, stdout, stderr):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        result = {
            "config": config["name"],
            "model_type": config["model_type"],
            "physics_type": config["physics_type"],
            "lambda_physics": config["lambda_physics"],
            "use_curriculum": config.get("use_curriculum", False),
            "success": success,
            "duration": duration,
            "timestamp": datetime.now().isoformat(),
            "stdout": stdout,
            "stderr": stderr
        }
        
        # æ·»åŠ è¯¾ç¨‹å­¦ä¹ ç›¸å…³å‚æ•°
        if config.get("use_curriculum", False):
            result.update({
                "warmup_epochs": config.get("warmup_epochs", 5),
                "ramp_epochs": config.get("ramp_epochs", 10),
                "max_lambda": config.get("max_lambda", config["lambda_physics"])
            })
        
        # ä¿å­˜åˆ°æ¨¡å‹ç›®å½•
        result_file = config["model_dir"] / "training_result.json"
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
            
        # ä¿å­˜åˆ°æ±‡æ€»æ—¥å¿—
        log_dir = self.base_dir / "training_logs"
        log_dir.mkdir(exist_ok=True)
        
        log_file = log_dir / f"training_log_{datetime.now().strftime('%Y%m%d')}.jsonl"
        with open(log_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(result, ensure_ascii=False) + '\n')
    
    def train_all_models(self, start_from=None, models_to_train=None):
        """ä¸²è¡Œè®­ç»ƒæ‰€æœ‰æ¨¡å‹"""
        print(f"ğŸ¯ å‡†å¤‡è®­ç»ƒæ¨¡å‹")
        print(f"ğŸ“Š æ€»è®¡æ¨¡å‹æ•°é‡: {len(self.training_configs)}")
        print(f"ğŸ”„ è®­ç»ƒæ–¹å¼: ä¸²è¡Œ (ä¸€ä¸ªæ¥ä¸€ä¸ª)")
        
        # æ˜¾ç¤ºæ‰€æœ‰é…ç½®
        print(f"\nğŸ“‹ è®­ç»ƒé…ç½®åˆ—è¡¨:")
        for i, config in enumerate(self.training_configs, 1):
            print(f"  {i:2d}. {config['name']} - {config['model_type'].upper()}, {config['physics_type']}, Î»={config['lambda_physics']}")
        
        # è¿‡æ»¤è¦è®­ç»ƒçš„æ¨¡å‹
        configs_to_train = self.training_configs.copy()
        
        if models_to_train:
            configs_to_train = [c for c in configs_to_train if c["name"] in models_to_train]
            print(f"\nğŸ¯ ä»…è®­ç»ƒæŒ‡å®šæ¨¡å‹: {models_to_train}")
            
        if start_from:
            start_idx = next((i for i, c in enumerate(configs_to_train) if c["name"] == start_from), 0)
            configs_to_train = configs_to_train[start_idx:]
            print(f"\nğŸ“ ä»æ¨¡å‹ '{start_from}' å¼€å§‹è®­ç»ƒ")
        
        if not configs_to_train:
            print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°è¦è®­ç»ƒçš„æ¨¡å‹!")
            return
            
        print(f"\nğŸš€ å¼€å§‹ä¸²è¡Œè®­ç»ƒ {len(configs_to_train)} ä¸ªæ¨¡å‹...")
        
        # å¼€å§‹è®­ç»ƒ
        successful_trainings = 0
        failed_trainings = 0
        
        for i, config in enumerate(configs_to_train, 1):
            print(f"\nğŸ”¢ è¿›åº¦: {i}/{len(configs_to_train)}")
            
            if self.train_model(config):
                successful_trainings += 1
            else:
                failed_trainings += 1
                
            # è®­ç»ƒé—´éš”
            if i < len(configs_to_train):
                print(f"â¸ï¸  ç­‰å¾… 5 ç§’åå¼€å§‹ä¸‹ä¸€ä¸ªæ¨¡å‹è®­ç»ƒ...")
                time.sleep(5)
        
        # è®­ç»ƒæ€»ç»“
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ è®­ç»ƒå®Œæˆæ€»ç»“")
        print(f"{'='*60}")
        print(f"âœ… æˆåŠŸè®­ç»ƒ: {successful_trainings}")
        print(f"âŒ å¤±è´¥è®­ç»ƒ: {failed_trainings}")
        print(f"ğŸ“Š æ€»è®¡æ¨¡å‹: {successful_trainings + failed_trainings}")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {successful_trainings/(successful_trainings + failed_trainings)*100:.1f}%")
        print(f"ğŸ• å®Œæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    def list_configs(self):
        """åˆ—å‡ºæ‰€æœ‰è®­ç»ƒé…ç½®"""
        print("ğŸ“‹ å¯ç”¨çš„è®­ç»ƒé…ç½®:")
        for i, config in enumerate(self.training_configs, 1):
            print(f"  {i:2d}. {config['name']}")
            print(f"      æ¨¡å‹ç±»å‹: {config['model_type'].upper()}")
            print(f"      ç‰©ç†é™åˆ¶: {config['physics_type']}")
            
            # æ˜¾ç¤ºlambdaä¿¡æ¯
            if config.get("use_curriculum", False):
                print(f"      è¯¾ç¨‹å­¦ä¹ : å¯ç”¨ (max_Î»={config.get('max_lambda', config['lambda_physics'])})")
                print(f"      çƒ­èº«æœŸ: {config.get('warmup_epochs', 5)} è½®")
                print(f"      å¢é•¿æœŸ: {config.get('ramp_epochs', 10)} è½®")
            else:
                print(f"      Lambdaå€¼: {config['lambda_physics']}")
                
            print(f"      æ¨¡å‹ç›®å½•: {config['model_dir']}")
            print()

def main():
    parser = argparse.ArgumentParser(description="RouteNetæ¨¡å‹ä¸²è¡Œè®­ç»ƒè„šæœ¬")
    parser.add_argument("--list", action="store_true", help="åˆ—å‡ºæ‰€æœ‰è®­ç»ƒé…ç½®")
    parser.add_argument("--start-from", type=str, help="ä»æŒ‡å®šæ¨¡å‹å¼€å§‹è®­ç»ƒ")
    parser.add_argument("--models", nargs="+", help="ä»…è®­ç»ƒæŒ‡å®šçš„æ¨¡å‹")
    parser.add_argument("--base-dir", default="./", help="é¡¹ç›®æ ¹ç›®å½•")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶é‡æ–°è®­ç»ƒå·²å­˜åœ¨çš„æ¨¡å‹")
    parser.add_argument("--yes", "-y", action="store_true", help="è‡ªåŠ¨ç¡®è®¤è®­ç»ƒï¼Œæ— éœ€æ‰‹åŠ¨è¾“å…¥")
    # æ—©åœç›¸å…³å‚æ•°
    parser.add_argument("--no-early-stopping", action="store_true", help="ç¦ç”¨æ—©åœæœºåˆ¶")
    parser.add_argument("--early-stopping-patience", type=int, default=5, help="æ—©åœè€å¿ƒå€¼ (é»˜è®¤: 5)")
    
    args = parser.parse_args()
    
    # åˆ›å»ºtrainerï¼Œä¼ å…¥æ—©åœç›¸å…³å‚æ•°
    trainer = ModelTrainer(
        base_dir=args.base_dir, 
        force_retrain=args.force,
        enable_early_stopping=not args.no_early_stopping,
        early_stopping_patience=args.early_stopping_patience
    )
    
    if args.list:
        trainer.list_configs()
        return
    
    # æ˜¾ç¤ºè®­ç»ƒæ¨¡å¼
    if args.force:
        print("ğŸ”„ å¼ºåˆ¶é‡æ–°è®­ç»ƒæ¨¡å¼ï¼šå°†è¦†ç›–å·²å­˜åœ¨çš„æ¨¡å‹")
    else:
        print("â­ï¸  è·³è¿‡æ¨¡å¼ï¼šå·²å­˜åœ¨çš„æ¨¡å‹å°†è¢«è·³è¿‡")
    
    # æ˜¾ç¤ºæ—©åœè®¾ç½®
    if trainer.enable_early_stopping:
        print(f"ğŸ›‘ æ—©åœæœºåˆ¶ï¼šå¯ç”¨ (è€å¿ƒå€¼: {trainer.early_stopping_patience})")
    else:
        print("ğŸ›‘ æ—©åœæœºåˆ¶ï¼šç¦ç”¨")
    
    # ç¡®è®¤å¼€å§‹è®­ç»ƒ
    if not args.models and not args.start_from and not args.yes:
        response = input(f"\nç¡®è®¤å¼€å§‹è®­ç»ƒæ‰€æœ‰ {len(trainer.training_configs)} ä¸ªæ¨¡å‹? (y/N): ")
        if response.lower() != 'y':
            print("âŒ å–æ¶ˆè®­ç»ƒ")
            return
    elif args.yes:
        print(f"âœ… è‡ªåŠ¨ç¡®è®¤è®­ç»ƒæ‰€æœ‰ {len(trainer.training_configs)} ä¸ªæ¨¡å‹")
    
    trainer.train_all_models(start_from=args.start_from, models_to_train=args.models)

if __name__ == "__main__":
    main()
