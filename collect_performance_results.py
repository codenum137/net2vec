#!/usr/bin/env python3
"""
æ”¶é›†å’Œæ±‡æ€»ä¸åŒå‚æ•°æ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡
ç”Ÿæˆæ¸…æ™°çš„æ€§èƒ½å¯¹æ¯”è¡¨æ ¼
"""

import os
import pandas as pd
import numpy as np
from pathlib import Path
import re

def extract_model_info(model_name):
    """
    ä»æ¨¡å‹åç§°ä¸­æå–æ¨¡å‹ç±»å‹å’Œç‰©ç†çº¦æŸå‚æ•°ä¿¡æ¯
    ä¾‹å¦‚: kan_hard_0.1 -> (KAN, Hard, 0.1)
         mlp_none -> (MLP, None, 0.0)
    """
    parts = model_name.split('_')
    
    if len(parts) == 2:  # kan_none, mlp_none
        model_type = parts[0].upper()
        physics_type = "None"
        lambda_physics = 0.0
    elif len(parts) == 3:  # kan_hard_0.1, mlp_soft_0.5
        model_type = parts[0].upper()
        physics_type = parts[1].capitalize()  # Hard, Soft
        lambda_physics = float(parts[2])
    else:
        return None, None, None
    
    return model_type, physics_type, lambda_physics

def clean_numeric_data(value):
    """
    æ¸…ç†æ•°å€¼æ•°æ®ï¼Œå¤„ç†ç™¾åˆ†æ¯”å’Œæ•°å€¼æ··åˆçš„æƒ…å†µ
    """
    if pd.isna(value):
        return np.nan
    
    value_str = str(value).strip()
    
    # å¤„ç†ç™¾åˆ†æ¯”
    if value_str.endswith('%'):
        return float(value_str[:-1].replace('+', '').replace('-', ''))
    
    # å¤„ç†æ™®é€šæ•°å€¼
    try:
        return float(value_str.replace('+', ''))
    except:
        return np.nan

def collect_performance_data(base_dir):
    """
    æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½æ•°æ®
    """
    base_path = Path(base_dir)
    all_data = []
    
    # æŸ¥æ‰¾æ‰€æœ‰numerical_performance_analysis.csvæ–‡ä»¶
    csv_files = list(base_path.glob("*/numerical/numerical_performance_analysis.csv"))
    
    print(f"æ‰¾åˆ° {len(csv_files)} ä¸ªæ€§èƒ½åˆ†ææ–‡ä»¶")
    
    for csv_file in csv_files:
        # ä»è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
        model_name = csv_file.parent.parent.name
        
        # è§£ææ¨¡å‹ä¿¡æ¯
        model_type, physics_type, lambda_physics = extract_model_info(model_name)
        
        if model_type is None:
            print(f"âš ï¸  è·³è¿‡æ— æ³•è§£æçš„æ¨¡å‹: {model_name}")
            continue
        
        try:
            # è¯»å–CSVæ–‡ä»¶
            df = pd.read_csv(csv_file)
            
            # ä¸ºæ¯ä¸€è¡Œæ·»åŠ æ¨¡å‹ä¿¡æ¯
            df['Model_Type'] = model_type
            df['Physics_Type'] = physics_type
            df['Lambda_Physics'] = lambda_physics
            df['Model_Name'] = model_name
            
            all_data.append(df)
            print(f"âœ… æˆåŠŸåŠ è½½: {model_name}")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥ {model_name}: {e}")
    
    if not all_data:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    combined_df = pd.concat(all_data, ignore_index=True)
    
    return combined_df

def create_performance_summary(df):
    """
    åˆ›å»ºæ€§èƒ½æ±‡æ€»è¡¨æ ¼
    """
    # é‡æ–°æ’åˆ—åˆ—çš„é¡ºåºï¼ŒæŠŠæ¨¡å‹ä¿¡æ¯æ”¾åœ¨å‰é¢
    columns_order = ['Model_Type', 'Physics_Type', 'Lambda_Physics', 'Model_Name', 
                    'Target', 'Metric', 'NSFNet (Training Topology)', 
                    'GBN (Test Topology)', 'Degradation']
    
    df_reordered = df[columns_order]
    
    return df_reordered

def create_pivot_tables(df):
    """
    åˆ›å»ºé€è§†è¡¨ï¼Œä¾¿äºå¯¹æ¯”åˆ†æ
    """
    # æ¸…ç†æ•°æ®
    df_clean = df.copy()
    
    # æ¸…ç†æ•°å€¼åˆ—
    numeric_columns = ['NSFNet (Training Topology)', 'GBN (Test Topology)']
    for col in numeric_columns:
        df_clean[f'{col}_numeric'] = df_clean[col].apply(clean_numeric_data)
    
    # æ¸…ç†é€€åŒ–æ•°æ®
    df_clean['Degradation_Numeric'] = df_clean['Degradation'].apply(clean_numeric_data)
    
    pivot_tables = {}
    
    # 1. æŒ‰æ¨¡å‹ç±»å‹å’ŒæŒ‡æ ‡çš„è®­ç»ƒé›†æ€§èƒ½é€è§†è¡¨ï¼ˆä¿ç•™åŸå§‹å­—ç¬¦ä¸²æ ¼å¼ï¼‰
    pivot_train = df_clean.pivot_table(
        index=['Model_Type', 'Physics_Type', 'Lambda_Physics'],
        columns=['Target', 'Metric'],
        values='NSFNet (Training Topology)',
        aggfunc='first'
    )
    pivot_tables['Training_Performance'] = pivot_train
    
    # 2. æŒ‰æ¨¡å‹ç±»å‹å’ŒæŒ‡æ ‡çš„æµ‹è¯•é›†æ€§èƒ½é€è§†è¡¨ï¼ˆä¿ç•™åŸå§‹å­—ç¬¦ä¸²æ ¼å¼ï¼‰
    pivot_test = df_clean.pivot_table(
        index=['Model_Type', 'Physics_Type', 'Lambda_Physics'],
        columns=['Target', 'Metric'],
        values='GBN (Test Topology)',
        aggfunc='first'
    )
    pivot_tables['Test_Performance'] = pivot_test
    
    # 3. æŒ‰æ¨¡å‹ç±»å‹å’ŒæŒ‡æ ‡çš„æ€§èƒ½é€€åŒ–é€è§†è¡¨ï¼ˆæ•°å€¼æ ¼å¼ï¼‰
    pivot_degradation = df_clean.pivot_table(
        index=['Model_Type', 'Physics_Type', 'Lambda_Physics'],
        columns=['Target', 'Metric'],
        values='Degradation_Numeric',
        aggfunc='first'
    )
    pivot_tables['Performance_Degradation'] = pivot_degradation
    
    # 4. è®­ç»ƒé›†æ€§èƒ½é€è§†è¡¨ï¼ˆæ•°å€¼æ ¼å¼ï¼Œç”¨äºæ’åºï¼‰
    pivot_train_numeric = df_clean.pivot_table(
        index=['Model_Type', 'Physics_Type', 'Lambda_Physics'],
        columns=['Target', 'Metric'],
        values='NSFNet (Training Topology)_numeric',
        aggfunc='first'
    )
    pivot_tables['Training_Performance_Numeric'] = pivot_train_numeric
    
    # 5. æµ‹è¯•é›†æ€§èƒ½é€è§†è¡¨ï¼ˆæ•°å€¼æ ¼å¼ï¼Œç”¨äºæ’åºï¼‰
    pivot_test_numeric = df_clean.pivot_table(
        index=['Model_Type', 'Physics_Type', 'Lambda_Physics'],
        columns=['Target', 'Metric'],
        values='GBN (Test Topology)_numeric',
        aggfunc='first'
    )
    pivot_tables['Test_Performance_Numeric'] = pivot_test_numeric
    
    return pivot_tables

def generate_performance_ranking(df):
    """
    ç”Ÿæˆæ€§èƒ½æ’åè¡¨
    """
    rankings = {}
    
    # æ¸…ç†æ•°æ®
    df_clean = df.copy()
    
    # æ¸…ç†æ•°å€¼åˆ—
    numeric_columns = ['NSFNet (Training Topology)', 'GBN (Test Topology)']
    for col in numeric_columns:
        df_clean[f'{col}_numeric'] = df_clean[col].apply(clean_numeric_data)
    
    # æ¸…ç†é€€åŒ–æ•°æ®
    df_clean['Degradation_Numeric'] = df_clean['Degradation'].apply(clean_numeric_data)
    
    # æŒ‰ä¸åŒæŒ‡æ ‡æ’åï¼ˆè¶Šå°è¶Šå¥½çš„æŒ‡æ ‡ï¼‰
    better_lower_metrics = ['MAE', 'RMSE', 'MAPE']
    # æŒ‰ä¸åŒæŒ‡æ ‡æ’åï¼ˆè¶Šå¤§è¶Šå¥½çš„æŒ‡æ ‡ï¼‰
    better_higher_metrics = ['R2']
    # ç‰¹æ®ŠæŒ‡æ ‡ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
    special_metrics = ['NLL']
    
    for target in df_clean['Target'].unique():
        for metric in df_clean['Metric'].unique():
            subset = df_clean[(df_clean['Target'] == target) & (df_clean['Metric'] == metric)]
            
            if subset.empty:
                continue
                
            key = f"{target}_{metric}"
            
            # ç¡®ä¿æœ‰æœ‰æ•ˆæ•°æ®
            train_col = 'NSFNet (Training Topology)_numeric'
            test_col = 'GBN (Test Topology)_numeric'
            
            valid_train = subset.dropna(subset=[train_col])
            valid_test = subset.dropna(subset=[test_col])
            
            if valid_train.empty or valid_test.empty:
                print(f"âš ï¸  è·³è¿‡ {key}: æ²¡æœ‰æœ‰æ•ˆæ•°å€¼æ•°æ®")
                continue
            
            if metric in better_lower_metrics or metric in special_metrics:
                # è®­ç»ƒé›†æ€§èƒ½æ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                train_ranking = valid_train.nsmallest(len(valid_train), train_col)[
                    ['Model_Name', 'Model_Type', 'Physics_Type', 'Lambda_Physics', 'NSFNet (Training Topology)', train_col]
                ].reset_index(drop=True)
                train_ranking.index = train_ranking.index + 1
                
                # æµ‹è¯•é›†æ€§èƒ½æ’åï¼ˆè¶Šå°è¶Šå¥½ï¼‰
                test_ranking = valid_test.nsmallest(len(valid_test), test_col)[
                    ['Model_Name', 'Model_Type', 'Physics_Type', 'Lambda_Physics', 'GBN (Test Topology)', test_col]
                ].reset_index(drop=True)
                test_ranking.index = test_ranking.index + 1
                
            elif metric in better_higher_metrics:
                # R2: è¶Šå¤§è¶Šå¥½
                train_ranking = valid_train.nlargest(len(valid_train), train_col)[
                    ['Model_Name', 'Model_Type', 'Physics_Type', 'Lambda_Physics', 'NSFNet (Training Topology)', train_col]
                ].reset_index(drop=True)
                train_ranking.index = train_ranking.index + 1
                
                test_ranking = valid_test.nlargest(len(valid_test), test_col)[
                    ['Model_Name', 'Model_Type', 'Physics_Type', 'Lambda_Physics', 'GBN (Test Topology)', test_col]
                ].reset_index(drop=True)
                test_ranking.index = test_ranking.index + 1
            
            rankings[f"{key}_Training"] = train_ranking
            rankings[f"{key}_Test"] = test_ranking
    
    return rankings

def save_results(df, pivot_tables, rankings, output_dir):
    """
    ä¿å­˜æ‰€æœ‰ç»“æœåˆ°æ–‡ä»¶
    """
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # 1. ä¿å­˜å®Œæ•´çš„æ±‡æ€»æ•°æ®
    summary_file = output_path / "performance_summary_complete.csv"
    df.to_csv(summary_file, index=False, encoding='utf-8')
    print(f"âœ… å®Œæ•´æ±‡æ€»æ•°æ®å·²ä¿å­˜: {summary_file}")
    
    # 2. ä¿å­˜é€è§†è¡¨
    for name, pivot_df in pivot_tables.items():
        pivot_file = output_path / f"pivot_{name.lower()}.csv"
        pivot_df.to_csv(pivot_file, encoding='utf-8')
        print(f"âœ… é€è§†è¡¨å·²ä¿å­˜: {pivot_file}")
    
    # 3. ä¿å­˜æ’åè¡¨
    ranking_dir = output_path / "rankings"
    ranking_dir.mkdir(exist_ok=True)
    
    for name, ranking_df in rankings.items():
        ranking_file = ranking_dir / f"ranking_{name.lower()}.csv"
        ranking_df.to_csv(ranking_file, encoding='utf-8')
        print(f"âœ… æ’åè¡¨å·²ä¿å­˜: {ranking_file}")
    
    # 4. ç”Ÿæˆå¯è¯»æ€§å¼ºçš„MarkdownæŠ¥å‘Š
    generate_markdown_report(df, pivot_tables, rankings, output_path)

def generate_markdown_report(df, pivot_tables, rankings, output_path):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„å¯è¯»æ€§æŠ¥å‘Š
    """
    report_file = output_path / "performance_analysis_report.md"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("# æ¨¡å‹æ€§èƒ½åˆ†ææŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        # 1. æ¦‚è§ˆ
        f.write("## 1. å®éªŒæ¦‚è§ˆ\n\n")
        total_models = df['Model_Name'].nunique()
        model_types = df['Model_Type'].unique()
        f.write(f"- æ€»æ¨¡å‹æ•°é‡: {total_models}\n")
        f.write(f"- æ¨¡å‹ç±»å‹: {', '.join(model_types)}\n")
        f.write(f"- è¯„ä¼°æŒ‡æ ‡: DELAY (MAE, RMSE, MAPE, R2, NLL), JITTER (MAE, RMSE, MAPE, R2)\n\n")
        
        # 2. æ¨¡å‹åˆ—è¡¨
        f.write("## 2. å®éªŒæ¨¡å‹åˆ—è¡¨\n\n")
        model_info = df.groupby(['Model_Type', 'Physics_Type', 'Lambda_Physics']).first()['Model_Name'].reset_index()
        f.write("| æ¨¡å‹ç±»å‹ | ç‰©ç†çº¦æŸç±»å‹ | Î»_physics | æ¨¡å‹åç§° |\n")
        f.write("|---------|-------------|-----------|----------|\n")
        for _, row in model_info.iterrows():
            f.write(f"| {row['Model_Type']} | {row['Physics_Type']} | {row['Lambda_Physics']} | {row['Model_Name']} |\n")
        f.write("\n")
        
        # 3. å…³é”®å‘ç°
        f.write("## 3. å…³é”®å‘ç°\n\n")
        
        # æ‰¾åˆ°æœ€ä½³æ€§èƒ½çš„æ¨¡å‹
        df_temp = df.copy()
        df_temp['GBN_numeric'] = df_temp['GBN (Test Topology)'].apply(clean_numeric_data)
        delay_mae_subset = df_temp[(df_temp['Target'] == 'DELAY') & (df_temp['Metric'] == 'MAE')].dropna(subset=['GBN_numeric'])
        
        if not delay_mae_subset.empty:
            delay_mae_best = delay_mae_subset.nsmallest(1, 'GBN_numeric')
            best_model = delay_mae_best.iloc[0]
            f.write(f"### ğŸ† DELAY MAE æœ€ä½³æ¨¡å‹\n")
            f.write(f"- **{best_model['Model_Name']}** (è®­ç»ƒé›†: {best_model['NSFNet (Training Topology)']}, æµ‹è¯•é›†: {best_model['GBN (Test Topology)']})\n\n")
        
        # æ‰¾åˆ°æ³›åŒ–æ€§èƒ½æœ€å¥½çš„æ¨¡å‹ï¼ˆé€€åŒ–æœ€å°ï¼‰
        df_clean = df.copy()
        df_clean['Degradation_Numeric'] = df_clean['Degradation'].apply(clean_numeric_data)
        delay_mae_generalization_subset = df_clean[(df_clean['Target'] == 'DELAY') & (df_clean['Metric'] == 'MAE')].dropna(subset=['Degradation_Numeric'])
        
        if not delay_mae_generalization_subset.empty:
            delay_mae_generalization = delay_mae_generalization_subset.nsmallest(1, 'Degradation_Numeric')
            best_gen_model = delay_mae_generalization.iloc[0]
            f.write(f"### ğŸ¯ DELAY MAE æ³›åŒ–æ€§èƒ½æœ€ä½³\n")
            f.write(f"- **{best_gen_model['Model_Name']}** (æ€§èƒ½é€€åŒ–: {best_gen_model['Degradation']})\n\n")
        
        # 4. è¯¦ç»†æ€§èƒ½è¡¨æ ¼
        f.write("## 4. è¯¦ç»†æ€§èƒ½å¯¹æ¯”\n\n")
        f.write("### 4.1 DELAYæŒ‡æ ‡å¯¹æ¯”\n\n")
        
        # ç®€åŒ–è¡¨æ ¼è¾“å‡º
        f.write("| æ¨¡å‹åç§° | ç±»å‹ | MAE(è®­ç»ƒ) | MAE(æµ‹è¯•) | RMSE(è®­ç»ƒ) | RMSE(æµ‹è¯•) | R2(è®­ç»ƒ) | R2(æµ‹è¯•) |\n")
        f.write("|----------|------|-----------|-----------|------------|-----------|----------|----------|\n")
        
        for model_name in sorted(df['Model_Name'].unique()):
            model_delay = df[(df['Model_Name'] == model_name) & (df['Target'] == 'DELAY')]
            if len(model_delay) > 0:
                model_type = model_delay.iloc[0]['Model_Type']
                
                mae_data = model_delay[model_delay['Metric'] == 'MAE']
                rmse_data = model_delay[model_delay['Metric'] == 'RMSE']
                r2_data = model_delay[model_delay['Metric'] == 'R2']
                
                mae_train = mae_data['NSFNet (Training Topology)'].values[0] if len(mae_data) > 0 else '-'
                mae_test = mae_data['GBN (Test Topology)'].values[0] if len(mae_data) > 0 else '-'
                rmse_train = rmse_data['NSFNet (Training Topology)'].values[0] if len(rmse_data) > 0 else '-'
                rmse_test = rmse_data['GBN (Test Topology)'].values[0] if len(rmse_data) > 0 else '-'
                r2_train = r2_data['NSFNet (Training Topology)'].values[0] if len(r2_data) > 0 else '-'
                r2_test = r2_data['GBN (Test Topology)'].values[0] if len(r2_data) > 0 else '-'
                
                f.write(f"| {model_name} | {model_type} | {mae_train} | {mae_test} | {rmse_train} | {rmse_test} | {r2_train} | {r2_test} |\n")
        
        f.write("\n")
        
        # 5. æ€§èƒ½é€€åŒ–åˆ†æ
        f.write("### 4.2 æ€§èƒ½é€€åŒ–åˆ†æï¼ˆDELAY MAEï¼‰\n\n")
        f.write("| æ’å | æ¨¡å‹åç§° | ç±»å‹ | è®­ç»ƒé›†MAE | æµ‹è¯•é›†MAE | æ€§èƒ½é€€åŒ– |\n")
        f.write("|------|----------|------|-----------|-----------|----------|\n")
        
        # æŒ‰æ€§èƒ½é€€åŒ–æ’åº
        delay_mae_degradation = df_clean[(df_clean['Target'] == 'DELAY') & (df_clean['Metric'] == 'MAE')].dropna(subset=['Degradation_Numeric'])
        delay_mae_degradation_sorted = delay_mae_degradation.nsmallest(len(delay_mae_degradation), 'Degradation_Numeric')
        
        for i, (_, row) in enumerate(delay_mae_degradation_sorted.iterrows(), 1):
            f.write(f"| {i} | {row['Model_Name']} | {row['Model_Type']} | {row['NSFNet (Training Topology)']} | {row['GBN (Test Topology)']} | {row['Degradation']} |\n")
        
        f.write("\n")
        
        # 6. ç»“è®ºå’Œå»ºè®®
        f.write("## 5. ç»“è®ºå’Œå»ºè®®\n\n")
        f.write("### æ¨¡å‹ç±»å‹å¯¹æ¯”\n")
        
        # KAN vs MLP ç»Ÿè®¡
        kan_models = df_clean[df_clean['Model_Type'] == 'KAN']
        mlp_models = df_clean[df_clean['Model_Type'] == 'MLP']
        
        if not kan_models.empty and not mlp_models.empty:
            kan_delay_mae = kan_models[(kan_models['Target'] == 'DELAY') & (kan_models['Metric'] == 'MAE')]['Degradation_Numeric']
            mlp_delay_mae = mlp_models[(mlp_models['Target'] == 'DELAY') & (mlp_models['Metric'] == 'MAE')]['Degradation_Numeric']
            
            if not kan_delay_mae.empty and not mlp_delay_mae.empty:
                kan_avg = kan_delay_mae.mean()
                mlp_avg = mlp_delay_mae.mean()
                f.write(f"- **KAN vs MLP**: KANå¹³å‡æ€§èƒ½é€€åŒ– {kan_avg:.2f}%, MLPå¹³å‡æ€§èƒ½é€€åŒ– {mlp_avg:.2f}%\n")
        
        f.write("- **ç‰©ç†çº¦æŸç­–ç•¥**: Hard vs Soft vs Noneçš„æ•ˆæœå¯¹æ¯”\n")
        f.write("- **Î»_physicså‚æ•°**: ä¸åŒç‰©ç†çº¦æŸå¼ºåº¦(0.1, 0.5, 1.0)çš„å½±å“åˆ†æ\n\n")
        
        f.write("### æ¨èé…ç½®\n")
        if not delay_mae_generalization_subset.empty:
            best_config = delay_mae_generalization_subset.nsmallest(3, 'Degradation_Numeric')
            f.write("**åŸºäºæ³›åŒ–æ€§èƒ½çš„å‰ä¸‰åæ¨è:**\n")
            for i, (_, row) in enumerate(best_config.iterrows(), 1):
                f.write(f"{i}. **{row['Model_Name']}** - æ€§èƒ½é€€åŒ–: {row['Degradation']}\n")
        f.write("\n")
    
    print(f"âœ… MarkdownæŠ¥å‘Šå·²ä¿å­˜: {report_file}")

def main():
    """ä¸»å‡½æ•°"""
    base_dir = "/home/ubantu/net2vec/experiment_results/opt"
    output_dir = "/home/ubantu/net2vec/experiment_results/opt/summary"
    
    try:
        print("ğŸš€ å¼€å§‹æ”¶é›†æ¨¡å‹æ€§èƒ½æ•°æ®...")
        
        # æ”¶é›†æ•°æ®
        df = collect_performance_data(base_dir)
        print(f"âœ… æˆåŠŸæ”¶é›† {len(df)} æ¡è®°å½•ï¼Œè¦†ç›– {df['Model_Name'].nunique()} ä¸ªæ¨¡å‹")
        
        # åˆ›å»ºæ±‡æ€»è¡¨æ ¼
        print("ğŸ“Š åˆ›å»ºæ€§èƒ½æ±‡æ€»è¡¨æ ¼...")
        df_summary = create_performance_summary(df)
        
        # åˆ›å»ºé€è§†è¡¨
        print("ğŸ“ˆ åˆ›å»ºé€è§†è¡¨...")
        pivot_tables = create_pivot_tables(df_summary)
        
        # ç”Ÿæˆæ’å
        print("ğŸ† ç”Ÿæˆæ€§èƒ½æ’å...")
        rankings = generate_performance_ranking(df_summary)
        
        # ä¿å­˜ç»“æœ
        print("ğŸ’¾ ä¿å­˜ç»“æœæ–‡ä»¶...")
        save_results(df_summary, pivot_tables, rankings, output_dir)
        
        print(f"\nğŸ‰ æ€§èƒ½åˆ†æå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {output_dir}")
        print(f"ğŸ“‹ ä¸»è¦æ–‡ä»¶:")
        print(f"   - performance_summary_complete.csv: å®Œæ•´æ±‡æ€»æ•°æ®")
        print(f"   - pivot_*.csv: é€è§†è¡¨")
        print(f"   - rankings/: æ€§èƒ½æ’å")
        print(f"   - performance_analysis_report.md: å¯è¯»æ€§æŠ¥å‘Š")
        
    except Exception as e:
        print(f"âŒ åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
