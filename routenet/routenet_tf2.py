# -*- coding: utf-8 -*-
import tensorflow as tf
import numpy as np
import argparse
import os
from tqdm import tqdm
import datetime
import random
from tensorboard.plugins.hparams import api as hp

# ==============================================================================
# KAN (Kolmogorov-Arnold Networks) Implementation
# ==============================================================================

class KANLayer(tf.keras.layers.Layer):
    """
    KAN (Kolmogorov-Arnold Networks) Layer implementation
    ç®€åŒ–ç‰ˆæœ¬ï¼Œé»˜è®¤ä½¿ç”¨å¤šé¡¹å¼åŸºï¼ˆ1, x, x^2, x^3ï¼‰ä½œä¸ºå¯å­¦ä¹ éçº¿æ€§ï¼›
    å¯é€‰å¯ç”¨ B æ ·æ¡åŸºå‡½æ•°ï¼ˆbsplineï¼‰ã€‚
    """
    
    def __init__(self, units, grid_size=5, spline_order=3, basis_type='poly', **kwargs):
        super(KANLayer, self).__init__(**kwargs)
        self.units = units
        self.grid_size = grid_size              # Bæ ·æ¡ç½‘æ ¼é—´éš”æ•°é‡ï¼ˆåŒºé—´æ•°ï¼‰
        self.spline_order = spline_order        # Bæ ·æ¡é˜¶æ¬¡ï¼ˆdegreeï¼‰ï¼Œå¸¸ç”¨3è¡¨ç¤ºä¸‰æ¬¡
        self.basis_type = basis_type            # 'poly' æˆ– 'bspline'
        # ä»¥ä¸‹åœ¨ build() ä¸­æ ¹æ® basis_type åŠ¨æ€ç¡®å®š
        self._basis_dim = None
        self._knots = None
        self._n_basis = None
        
    def build(self, input_shape):
        input_dim = int(input_shape[-1])
        
        # åŸºç¡€æƒé‡çŸ©é˜µï¼ˆçº¿æ€§éƒ¨åˆ†ï¼‰
        self.base_weight = self.add_weight(
            name='base_weight',
            shape=(input_dim, self.units),
            initializer='glorot_uniform',
            trainable=True
        )
        
        # åç½®é¡¹
        self.bias = self.add_weight(
            name='bias',
            shape=(self.units,),
            initializer='zeros',
            trainable=True
        )
        
        # ç¡®å®šåŸºå‡½æ•°ç»´åº¦ï¼Œå¹¶åœ¨éœ€è¦æ—¶å‡†å¤‡Bæ ·æ¡ç»“ç‚¹
        if self.basis_type == 'bspline':
            # Open uniform B-spline knots on [0, 1]
            import numpy as _np
            degree = int(self.spline_order)
            n_intervals = int(self.grid_size)
            # é€‰æ‹©åŸºå‡½æ•°æ•°é‡ï¼ˆæ§åˆ¶ç‚¹æ•°ï¼‰ï¼šn_basis = n_intervals + degree
            self._n_basis = n_intervals + degree
            # æ„é€  open-uniform knot å‘é‡: [0]*(degree+1), internal uniform, [1]*(degree+1)
            if n_intervals > 1:
                internal = _np.linspace(0.0, 1.0, n_intervals + 1, dtype=_np.float32)[1:-1]
            else:
                internal = _np.array([], dtype=_np.float32)
            start = _np.zeros((degree + 1,), dtype=_np.float32)
            end = _np.ones((degree + 1,), dtype=_np.float32)
            np_knots = _np.concatenate([start, internal, end], axis=0)  # len = n_basis + degree + 1
            # å°† knots æ³¨å†Œä¸ºä¸å¯è®­ç»ƒçš„å˜é‡ï¼Œé¿å… tf.function å›¾ä½œç”¨åŸŸé—®é¢˜
            self._knots = self.add_weight(
                name='bspline_knots',
                shape=(np_knots.shape[0],),
                dtype=tf.float32,
                initializer=tf.keras.initializers.Constant(np_knots),
                trainable=False
            )
            self._basis_dim = self._n_basis
        else:
            # å¤šé¡¹å¼åŸºï¼š1, x, x^2, x^3
            self._basis_dim = 4

        # æ ·æ¡/åŸºå‡½æ•°çš„æƒé‡å‚æ•°ï¼šæ¯ä¸ªè¾“å…¥-è¾“å‡ºè¿æ¥å¯¹åº” basis_dim ä¸ªç³»æ•°
        self.spline_weights = self.add_weight(
            name='spline_weights',
            shape=(input_dim, self.units, self._basis_dim),
            initializer='glorot_uniform',
            trainable=True
        )
        
        # é—¨æ§å‚æ•°ï¼šæ§åˆ¶çº¿æ€§éƒ¨åˆ†å’Œéçº¿æ€§éƒ¨åˆ†çš„æƒé‡
        self.gate_weights = self.add_weight(
            name='gate_weights',
            shape=(input_dim, self.units),
            initializer='ones',
            trainable=True
        )
        
        super(KANLayer, self).build(input_shape)
    
    def _bspline_basis(self, u):
        """
        è®¡ç®— B æ ·æ¡åŸºå‡½æ•°å€¼ã€‚
        è¾“å…¥:
          u: [batch_size, input_dim]ï¼Œå®šä¹‰åŸŸå‡è®¾åœ¨ [0, 1]
        è¿”å›:
          basis: [batch_size, input_dim, n_basis]
        """
        degree = int(self.spline_order)
        knots = self._knots  # [n_basis + degree + 1]
        n_basis = int(self._n_basis)

        # åŸºç¡€çš„0æ¬¡åŸºå‡½æ•° B_{i,0}
        # å¯¹æ¯ä¸ª i (0..n_basis-1): 1 if knots[i] <= u < knots[i+1] else 0
        # å¤„ç† u==1 çš„è¾¹ç•Œï¼ˆå½’å±åˆ°æœ€åä¸€ä¸ªåŸºå‡½æ•°ï¼‰
        u_exp = tf.expand_dims(u, axis=-1)  # [B, D, 1]
        t_i = tf.reshape(knots[:n_basis], [1, 1, n_basis])
        t_ip1 = tf.reshape(knots[1:n_basis+1], [1, 1, n_basis])

        left = tf.cast(u_exp >= t_i, tf.float32)
        right = tf.cast(u_exp < t_ip1, tf.float32)
        B = left * right  # [B, D, n_basis]

        # ç‰¹æ®Šå¤„ç† u==1.0: ä»¤æœ€åä¸€ä¸ªåŸºå‡½æ•°ä¸º1
        is_one = tf.equal(u_exp, 1.0)
        any_one = tf.cast(is_one, tf.float32)
        last_hot = tf.one_hot(n_basis - 1, n_basis, dtype=tf.float32)  # [n_basis]
        last_hot = tf.reshape(last_hot, [1, 1, n_basis])
        B = tf.where(tf.reduce_any(is_one, axis=-1, keepdims=True), last_hot, B)

        # é€’æ¨è®¡ç®—é«˜é˜¶åŸºå‡½æ•°
        for k in range(1, degree + 1):
            # denom1: t_{i+k} - t_i,  i=0..n_basis-1
            t_i_k = tf.reshape(knots[k:n_basis + k], [1, 1, n_basis])
            denom1 = t_i_k - t_i
            # denom2: t_{i+k+1} - t_{i+1}
            t_ip1_k1 = tf.reshape(knots[k+1:n_basis + k + 1], [1, 1, n_basis])
            denom2 = t_ip1_k1 - t_ip1

            # term1 = ((u - t_i)/denom1) * B_{i,k-1}
            numer1 = u_exp - t_i
            coef1 = tf.where(denom1 > 0, numer1 / (denom1 + 1e-12), tf.zeros_like(denom1))
            term1 = coef1 * B

            # term2 = ((t_{i+k+1} - u)/denom2) * B_{i+1,k-1}
            numer2 = t_ip1_k1 - u_exp
            coef2 = tf.where(denom2 > 0, numer2 / (denom2 + 1e-12), tf.zeros_like(denom2))
            # B shifted: B_{i+1,k-1}
            B_shift = tf.concat([B[..., 1:], tf.zeros_like(B[..., :1])], axis=-1)
            term2 = coef2 * B_shift

            B = term1 + term2

        return B  # [B, D, n_basis]

    def call(self, inputs, training=None):
        # åŸºç¡€çº¿æ€§å˜æ¢
        linear_output = tf.matmul(inputs, self.base_weight) + self.bias  # [batch_size, units]
        
        # éçº¿æ€§å˜æ¢ï¼šå¤šé¡¹å¼æˆ–Bæ ·æ¡
        # å°†è¾“å…¥æ ‡å‡†åŒ–
        x_tanh = tf.tanh(inputs)  # [-1, 1]
        if self.basis_type == 'bspline':
            # remap to [0,1]
            u = (x_tanh + 1.0) * 0.5
            basis = self._bspline_basis(u)  # [B, D, n_basis]
        else:
            # å¤šé¡¹å¼åŸºå‡½æ•°ï¼š1, x, x^2, x^3
            basis = tf.stack([
                tf.ones_like(x_tanh),
                x_tanh,
                tf.square(x_tanh),
                tf.pow(x_tanh, 3)
            ], axis=-1)  # [B, D, 4]
        
        # æ ·æ¡/åŸºå‡½æ•°è¾“å‡ºï¼š[B, D, basis] @ [D, U, basis] -> [B, D, U]
        spline_contributions = tf.einsum('bid,ijd->bij', basis, self.spline_weights)  # [batch_size, input_dim, units]
        
        # åº”ç”¨é—¨æ§æƒé‡å¹¶æ±‚å’Œ
        gated_splines = spline_contributions * tf.expand_dims(self.gate_weights, 0)  # [batch_size, input_dim, units]
        spline_output = tf.reduce_sum(gated_splines, axis=1)  # [batch_size, units]
        
        # ç»„åˆçº¿æ€§å’Œéçº¿æ€§éƒ¨åˆ†
        output = linear_output + spline_output
        
        # åº”ç”¨æ¿€æ´»å‡½æ•°
        output = tf.nn.selu(output)
        
        return output
    
    def compute_output_shape(self, input_shape):
        return (input_shape[0], self.units)
    
    def get_config(self):
        config = super(KANLayer, self).get_config()
        config.update({
            'units': self.units,
            'grid_size': self.grid_size,
            'spline_order': self.spline_order,
            'basis_type': self.basis_type,
        })
        return config

# ==============================================================================
# 1. æ•°æ®åŠ è½½ä¸é¢„å¤„ç† (æ— å˜åŒ–)
# ==============================================================================

def scale_fn(k, val):
    if k == 'traffic':
        return (val - 0.18) / 0.15
    if k == 'capacities':
        return val / 10.0
    return val

def parse_fn(serialized):
    feature_spec = {
        'traffic': tf.io.VarLenFeature(tf.float32),
        'delay': tf.io.VarLenFeature(tf.float32),
        'jitter': tf.io.VarLenFeature(tf.float32),
        'drops': tf.io.VarLenFeature(tf.float32),
        'packets': tf.io.VarLenFeature(tf.float32),
        'capacities': tf.io.VarLenFeature(tf.float32),
        'links': tf.io.VarLenFeature(tf.int64),
        'paths': tf.io.VarLenFeature(tf.int64),
        'sequences': tf.io.VarLenFeature(tf.int64),
        'n_links': tf.io.FixedLenFeature([], tf.int64),
        'n_paths': tf.io.FixedLenFeature([], tf.int64),
    }
    
    features = tf.io.parse_single_example(serialized, features=feature_spec)
    
    # è½¬æ¢ç¨€ç–å¼ é‡å¹¶æ ‡å‡†åŒ–
    for k, v in features.items():
        if isinstance(v, tf.SparseTensor):
            v = tf.sparse.to_dense(v)
        if k in ['traffic', 'capacities']:
            v = scale_fn(k, v)
        features[k] = v
            
    labels = {
        'delay': features.pop('delay'),
        'jitter': features.pop('jitter'),
        'drops': features.pop('drops'),
        'packets': features['packets']
    }
    return features, labels

def transformation_func(features_batch, labels_batch):
    """æ­£ç¡®çš„å›¾åˆå¹¶æ‰¹å¤„ç†å‡½æ•°ï¼Œä¿®å¤å½¢çŠ¶ä¸åŒ¹é…é—®é¢˜"""
    
    # è·å–æ‰¹æ¬¡å¤§å°
    batch_size = tf.shape(features_batch['n_links'])[0]
    
    # è®¡ç®—ç´¯ç§¯åç§»é‡
    n_links_cumsum = tf.cumsum(features_batch['n_links'])
    n_paths_cumsum = tf.cumsum(features_batch['n_paths'])
    
    link_offsets = tf.concat([[0], n_links_cumsum[:-1]], axis=0)
    path_offsets = tf.concat([[0], n_paths_cumsum[:-1]], axis=0)
    
    # ä½¿ç”¨ tf.map_fn æ¥æ­£ç¡®å¤„ç†æ¯ä¸ªæ ·æœ¬çš„åç§»é‡
    def apply_link_offset(args):
        i, links = args
        return links + link_offsets[i]
    
    def apply_path_offset(args):
        i, paths = args
        return paths + path_offsets[i]
    
    # ä¸ºæ¯ä¸ªæ ·æœ¬åº”ç”¨å¯¹åº”çš„åç§»é‡
    batch_indices = tf.range(batch_size)
    
    adjusted_links = tf.map_fn(
        apply_link_offset,
        (batch_indices, features_batch['links']),
        fn_output_signature=tf.RaggedTensorSpec(shape=[None], dtype=tf.int64),
        parallel_iterations=10
    )
    
    adjusted_paths = tf.map_fn(
        apply_path_offset,
        (batch_indices, features_batch['paths']),
        fn_output_signature=tf.RaggedTensorSpec(shape=[None], dtype=tf.int64),
        parallel_iterations=10
    )
    
    # åˆå¹¶æ‰€æœ‰ç‰¹å¾ - ä½¿ç”¨ flat_values è·å–å±•å¹³çš„æ•°æ®
    merged_features = {
        'traffic': features_batch['traffic'].flat_values,
        'capacities': features_batch['capacities'].flat_values,
        'packets': features_batch['packets'].flat_values,
        'links': adjusted_links.flat_values,
        'paths': adjusted_paths.flat_values,
        'sequences': features_batch['sequences'].flat_values,
        'n_links': tf.reduce_sum(features_batch['n_links']),
        'n_paths': tf.reduce_sum(features_batch['n_paths']),
    }
    
    merged_labels = {
        'delay': labels_batch['delay'].flat_values,
        'jitter': labels_batch['jitter'].flat_values,
        'drops': labels_batch['drops'].flat_values,
        'packets': labels_batch['packets'].flat_values,
    }
    
    return merged_features, merged_labels

def create_dataset(filenames, batch_size, is_training=True):
    ds = tf.data.TFRecordDataset(filenames)
    if is_training:
        ds = ds.shuffle(1000)
    
    ds = ds.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)
    
    # ä½¿ç”¨ ragged_batch æ¥å¤„ç†å¯å˜é•¿åº¦çš„å¼ é‡
    ds = ds.ragged_batch(batch_size)
    
    # ç„¶ååº”ç”¨å›¾åˆå¹¶å‡½æ•°
    ds = ds.map(transformation_func, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.prefetch(tf.data.AUTOTUNE)
    return ds

# ==============================================================================
# 2. RouteNet æ¨¡å‹ (æ”¯æŒä¸åŒçš„è¾“å‡ºé…ç½®)
# ==============================================================================

class RouteNet(tf.keras.Model):
    def __init__(self, config, output_units=2, final_activation=None, use_kan=False):
        super().__init__()
        self.config = config
        self.use_kan = use_kan
        
        # æ¶ˆæ¯ä¼ é€’å±‚ï¼ˆä¸åŸç‰ˆä¿æŒä¸€è‡´çš„å‘½åï¼‰
        self.link_update = tf.keras.layers.GRUCell(config['link_state_dim'])  # åŸç‰ˆä¸­å« edge_update
        self.path_update = tf.keras.layers.GRUCell(config['path_state_dim'])
        
        # ã€ä¿®å¤1: åœ¨__init__ä¸­åˆ›å»ºRNNå±‚ï¼Œé¿å…åœ¨å¾ªç¯ä¸­é‡å¤åˆ›å»ºã€‘
        self.rnn_layer = tf.keras.layers.RNN(
            self.path_update, 
            return_sequences=True, 
            return_state=True
        )
        
        # è¯»å‡ºç½‘ç»œï¼ˆæ”¯æŒKANå’Œä¼ ç»ŸMLPï¼‰
        dropout_rate = config.get('dropout_rate', 0.1)
        if use_kan:
            readout_layers = []
            for i in range(config['readout_layers']):
                readout_layers.append(KANLayer(
                    config['readout_units'],
                    grid_size=config.get('kan_grid_size', 5),
                    spline_order=config.get('kan_spline_order', 3),
                    basis_type=config.get('kan_basis', 'poly')
                ))
                if dropout_rate > 0 and i < config['readout_layers'] - 1:  # ä¸åœ¨æœ€åä¸€å±‚æ·»åŠ dropout
                    readout_layers.append(tf.keras.layers.Dropout(dropout_rate))
            self.readout = tf.keras.Sequential(readout_layers)
        else:
            # ä¼ ç»Ÿ MLP è¯»å‡ºç½‘ç»œ
            readout_layers = []
            for _ in range(config['readout_layers']):
                readout_layers.append(tf.keras.layers.Dense(
                    config['readout_units'], 
                    activation='selu',
                    kernel_regularizer=tf.keras.regularizers.l2(config['l2'])
                ))
                if dropout_rate > 0:
                    readout_layers.append(tf.keras.layers.Dropout(dropout_rate))
            self.readout = tf.keras.Sequential(readout_layers)
        
        # æœ€ç»ˆè¾“å‡ºå±‚ï¼Œæ”¯æŒä¸åŒçš„æ¿€æ´»å‡½æ•°
        self.final_layer = tf.keras.layers.Dense(
            output_units,
            activation=final_activation,
            kernel_regularizer=tf.keras.regularizers.l2(config['l2_2'])
        )

    def call(self, inputs, training=False):
        # åˆå§‹åŒ–çŠ¶æ€
        link_state = tf.concat([
            tf.expand_dims(inputs['capacities'], axis=1),
            tf.zeros([inputs['n_links'], self.config['link_state_dim'] - 1])
        ], axis=1)
        
        path_state = tf.concat([
            tf.expand_dims(inputs['traffic'], axis=1),
            tf.zeros([inputs['n_paths'], self.config['path_state_dim'] - 1])
        ], axis=1)

        links = inputs['links']
        paths = inputs['paths']
        seqs = inputs['sequences']
        
        # T è½®æ¶ˆæ¯ä¼ é€’ï¼ˆä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„ RNN å¤„ç†ï¼‰
        for _ in range(self.config['T']):
            # æ”¶é›†æ¯æ¡è¾¹ä¸Šçš„é“¾è·¯çŠ¶æ€
            h_ = tf.gather(link_state, links)
            
            # æ„å»ºè·¯å¾„çš„åºåˆ—è¾“å…¥ - ä¸åŸç‰ˆå®Œå…¨ä¸€è‡´
            ids = tf.stack([paths, seqs], axis=1)
            max_len = tf.reduce_max(seqs) + 1
            shape = tf.stack([inputs['n_paths'], max_len, self.config['link_state_dim']])
            
            # è®¡ç®—æ¯æ¡è·¯å¾„çš„é•¿åº¦
            # æ³¨æ„ï¼šsegment_sum è¦æ±‚ segment_ids æ˜¯æ’åºçš„
            unique_paths, _ = tf.unique(paths)
            lens = tf.math.unsorted_segment_sum(
                data=tf.ones_like(paths, dtype=tf.int32),
                segment_ids=paths, 
                num_segments=inputs['n_paths']
            )
            
            # å°†é“¾è·¯çŠ¶æ€æ•£å¸ƒåˆ°åºåˆ—æ ¼å¼ [n_paths, max_len, link_state_dim]
            link_inputs = tf.scatter_nd(ids, h_, shape)
            
            # ä½¿ç”¨ masking æ¥å¤„ç†å˜é•¿åºåˆ—
            # åˆ›å»º mask: True è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼ŒFalse è¡¨ç¤º padding
            mask = tf.sequence_mask(lens, maxlen=max_len, dtype=tf.bool)
            
            # ã€ä¿®å¤: ä½¿ç”¨é¢„å…ˆåˆ›å»ºçš„RNNå±‚ï¼Œè€Œä¸æ˜¯åœ¨å¾ªç¯ä¸­é‡å¤åˆ›å»ºã€‘
            # RNN å‰å‘ä¼ æ’­
            outputs, path_state = self.rnn_layer(
                link_inputs, 
                initial_state=path_state, 
                mask=mask,
                training=training
            )
            
            # ä» RNN è¾“å‡ºä¸­æå–å¯¹åº”è·¯å¾„ä½ç½®çš„ç»“æœ
            m = tf.gather_nd(outputs, ids)
            
            # æŒ‰é“¾è·¯èšåˆæ‰€æœ‰è·¯å¾„çš„æ¶ˆæ¯
            m = tf.math.unsorted_segment_sum(m, links, inputs['n_links'])
            
            # æ›´æ–°é“¾è·¯çŠ¶æ€
            link_state, _ = self.link_update(m, [link_state])

        # è¯»å‡ºé˜¶æ®µ
        readout_output = self.readout(path_state, training=training)
        final_input = tf.concat([readout_output, path_state], axis=1)
        return self.final_layer(final_input)

# ==============================================================================
# é«˜æ•ˆç‰©ç†çº¦æŸæ¨¡å‹ - è§£å†³å†—ä½™å‰å‘ä¼ æ’­é—®é¢˜
# ==============================================================================

class PhysicsInformedRouteNet(tf.keras.Model):
    """
    ç‰©ç†çº¦æŸRouteNetæ¨¡å‹ - é«˜æ•ˆå®ç° + è¯¾ç¨‹å­¦ä¹ 
    
    é€šè¿‡è‡ªå®šä¹‰train_stepæ–¹æ³•ï¼Œåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­åŒæ—¶è®¡ç®—ï¼š
    1. æ ‡å‡†é¢„æµ‹æŸå¤± (L_hetero æˆ– L_binomial)
    2. ç‰©ç†çº¦æŸæŸå¤± (L_gradient)
    
    æ”¯æŒè¯¾ç¨‹å­¦ä¹ ç­–ç•¥ï¼š
    - çƒ­èº«æœŸï¼šlambda=0ï¼Œä¸“æ³¨æ•°æ®æ‹Ÿåˆ
    - å¢é•¿æœŸï¼šlambdaçº¿æ€§å¢é•¿åˆ°max_lambda
    - ä¿æŒæœŸï¼šlambdaä¿æŒåœ¨max_lambda
    
    è¿™æ¶ˆé™¤äº†åŸå§‹å®ç°ä¸­çš„å†—ä½™å‰å‘ä¼ æ’­ï¼Œæ˜¾è‘—æå‡è®­ç»ƒæ•ˆç‡ã€‚
    """
    
    def __init__(self, config, target='delay', use_kan=False, 
                 use_physics_loss=False, use_hard_constraint=True, lambda_physics=0.1,
                 use_curriculum=False, warmup_epochs=5, ramp_epochs=10, max_lambda=0.1):
        super(PhysicsInformedRouteNet, self).__init__()
        
        self.config = config
        self.target = target
        self.use_kan = use_kan
        self.use_physics_loss = use_physics_loss
        self.use_hard_constraint = use_hard_constraint
        
        # è¯¾ç¨‹å­¦ä¹ å‚æ•°
        self.use_curriculum = use_curriculum
        if use_curriculum:
            self.warmup_epochs = warmup_epochs
            self.ramp_epochs = ramp_epochs
            self.max_lambda = max_lambda
            self.lambda_physics = tf.Variable(0.0, trainable=False, name='lambda_physics')
            self.current_lambda_physics = tf.Variable(0.0, trainable=False, name='current_lambda_physics')
            self.current_epoch = tf.Variable(0, trainable=False, name='current_epoch', dtype=tf.int32)
        else:
            self.lambda_physics = lambda_physics
            self.current_lambda_physics = lambda_physics
        
        # åˆ›å»ºæ ¸å¿ƒRouteNetæ¨¡å‹
        if target == 'delay':
            self.routenet = RouteNet(config, output_units=2, final_activation=None, use_kan=use_kan)
        else:  # drops
            self.routenet = RouteNet(config, output_units=1, final_activation=None, use_kan=use_kan)
        
        # æŸå¤±å‡½æ•°è¿½è¸ªæŒ‡æ ‡
        self.total_loss_tracker = tf.keras.metrics.Mean(name="total_loss")
        self.hetero_loss_tracker = tf.keras.metrics.Mean(name="hetero_loss")
        self.gradient_loss_tracker = tf.keras.metrics.Mean(name="gradient_loss")
        
        # è¯¾ç¨‹å­¦ä¹ è¿½è¸ªæŒ‡æ ‡
        if self.use_curriculum:
            self.lambda_tracker = tf.keras.metrics.Mean(name="lambda_physics_value")
        
        print(f"Created PhysicsInformedRouteNet:")
        print(f"  - Target: {target}")
        if use_kan:
            print(f"  - Architecture: KAN (basis={config.get('kan_basis', 'poly')}, grid={config.get('kan_grid_size', 5)}, order={config.get('kan_spline_order', 3)})")
        else:
            print(f"  - Architecture: MLP")
        print(f"  - Physics Loss: {use_physics_loss}")
        if use_physics_loss:
            constraint_type = "Hard" if use_hard_constraint else "Soft"
            print(f"  - Constraint Type: {constraint_type}")
            if use_curriculum:
                print(f"  - Curriculum Learning: Enabled")
                print(f"    * Warmup Epochs: {warmup_epochs}")
                print(f"    * Ramp-up Epochs: {ramp_epochs}")
                print(f"    * Max Lambda: {max_lambda}")
                print(f"    * Strategy: Linear Ramp-up")
            else:
                print(f"  - Lambda Physics: {lambda_physics}")

    def call(self, inputs, training=False):
        """å‰å‘ä¼ æ’­ - ç›´æ¥è°ƒç”¨å†…éƒ¨RouteNet"""
        return self.routenet(inputs, training=training)
    
    @property
    def metrics(self):
        """è¿”å›è·Ÿè¸ªçš„æŒ‡æ ‡"""
        metrics = [
            self.total_loss_tracker,
            self.hetero_loss_tracker,
            self.gradient_loss_tracker,
        ]
        if self.use_curriculum:
            metrics.append(self.lambda_tracker)
        return metrics

    def update_curriculum_lambda(self, epoch):
        """
        è¯¾ç¨‹å­¦ä¹ ï¼šåŠ¨æ€æ›´æ–°lambda_physicså€¼
        
        ç­–ç•¥ï¼šçº¿æ€§å¢é•¿ (Linear Ramp-up)
        - çƒ­èº«æœŸ (0 <= epoch < warmup_epochs): lambda = 0
        - å¢é•¿æœŸ (warmup_epochs <= epoch < warmup_epochs + ramp_epochs): çº¿æ€§å¢é•¿
        - ä¿æŒæœŸ (epoch >= warmup_epochs + ramp_epochs): lambda = max_lambda
        
        Args:
            epoch: å½“å‰è®­ç»ƒè½®æ•°ï¼ˆä»0å¼€å§‹ï¼‰
        """
        if not self.use_curriculum:
            return
            
        epoch = tf.cast(epoch, tf.float32)
        warmup_epochs = tf.cast(self.warmup_epochs, tf.float32)
        ramp_epochs = tf.cast(self.ramp_epochs, tf.float32)
        
        # çƒ­èº«æœŸï¼šlambda = 0
        warmup_condition = epoch < warmup_epochs
        
        # å¢é•¿æœŸï¼šçº¿æ€§å¢é•¿
        ramp_start = warmup_epochs
        ramp_end = warmup_epochs + ramp_epochs
        ramp_condition = tf.logical_and(epoch >= ramp_start, epoch < ramp_end)
        
        # è®¡ç®—çº¿æ€§å¢é•¿çš„lambdaå€¼
        # progress = (epoch - warmup_epochs) / ramp_epochs
        # lambda = max_lambda * progress
        progress = (epoch - ramp_start) / ramp_epochs
        ramp_lambda = self.max_lambda * progress
        
        # ä¿æŒæœŸï¼šlambda = max_lambda
        hold_condition = epoch >= ramp_end
        
        # ä½¿ç”¨tf.caseè¿›è¡Œæ¡ä»¶é€‰æ‹©
        new_lambda = tf.case([
            (warmup_condition, lambda: 0.0),
            (ramp_condition, lambda: ramp_lambda),
            (hold_condition, lambda: self.max_lambda)
        ], exclusive=True)
        
        # æ›´æ–°ä¸¤ä¸ªlambdaå˜é‡ï¼šlambda_physicsç”¨äºè®°å½•ï¼Œcurrent_lambda_physicsç”¨äºå®é™…è®¡ç®—
        self.lambda_physics.assign(new_lambda)
        self.current_lambda_physics.assign(new_lambda)
        self.current_epoch.assign(tf.cast(epoch, tf.int32))
        
        return new_lambda

    def get_current_lambda(self):
        """è·å–å½“å‰çš„lambda_physicså€¼"""
        if self.use_curriculum:
            return self.lambda_physics.numpy()
        else:
            return self.lambda_physics

    def _compute_hetero_loss(self, y_true, y_pred):
        """è®¡ç®—å¼‚æ–¹å·®æŸå¤±ï¼ˆå»¶è¿Ÿé¢„æµ‹ï¼‰"""
        loc = y_pred[:, 0]
        
        # ä¸åŸç‰ˆä¿æŒä¸€è‡´çš„scaleè®¡ç®—
        c = tf.math.log(tf.math.expm1(tf.constant(0.098, dtype=tf.float32)))
        scale = tf.nn.softplus(c + y_pred[:, 1]) + 1e-9
        
        delay_true = y_true['delay']
        jitter_true = y_true['jitter']
        packets_true = y_true['packets'] 
        drops_true = y_true['drops']
        
        n = packets_true - drops_true
        _2sigma = tf.constant(2.0, dtype=tf.float32) * tf.square(scale)
        
        nll = (n * jitter_true / _2sigma + 
               n * tf.square(delay_true - loc) / _2sigma + 
               n * tf.math.log(scale))
               
        return tf.reduce_sum(nll) / 1e6

    def _compute_binomial_loss(self, y_true, y_pred):
        """è®¡ç®—äºŒé¡¹åˆ†å¸ƒæŸå¤±ï¼ˆä¸¢åŒ…é¢„æµ‹ï¼‰"""
        logits = y_pred[:, 0]
        
        packets_true = y_true['packets']
        drops_true = y_true['drops']
        
        loss_ratio = drops_true / (packets_true + 1e-9)
        
        loss = tf.reduce_sum(
            packets_true * tf.nn.sigmoid_cross_entropy_with_logits(
                labels=loss_ratio,
                logits=logits
            )
        ) / 1e5
        
        return loss

    def call_with_gradients(self, features, training=False):
        """
        å•æ¬¡å‰å‘ä¼ æ’­åŒæ—¶è®¡ç®—é¢„æµ‹å’Œæ¢¯åº¦
        
        è¿™æ˜¯å…³é”®ä¼˜åŒ–ï¼šåœ¨ä¸€æ¬¡å‰å‘ä¼ æ’­ä¸­åŒæ—¶å¾—åˆ°ï¼š
        1. æ¨¡å‹é¢„æµ‹ predictions
        2. é¢„æµ‹ç›¸å¯¹äºtrafficçš„æ¢¯åº¦ gradients
        """
        traffic = features['traffic']
        
        with tf.GradientTape() as grad_tape:
            grad_tape.watch(traffic)
            predictions = self.routenet(features, training=training)
            
            # åªåœ¨éœ€è¦æ¢¯åº¦çº¦æŸæ—¶è®¡ç®—æ¢¯åº¦
            if self.use_physics_loss and self.target == 'delay' and predictions.shape[1] == 2:
                loc = predictions[:, 0]
            else:
                loc = None
        
        # è®¡ç®—æ¢¯åº¦ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if loc is not None:
            gradients = grad_tape.gradient(loc, traffic)
        else:
            gradients = None
        
        return predictions, gradients

    def _compute_gradient_loss_from_gradients(self, traffic_gradients):
        """ä»å·²è®¡ç®—çš„æ¢¯åº¦è®¡ç®—çº¦æŸæŸå¤±"""
        if traffic_gradients is None:
            return tf.constant(0.0, dtype=tf.float32)
        
        if self.use_hard_constraint:
            # ç¡¬çº¦æŸï¼šE_batch[ReLU(-gk)]
            gradient_penalties = tf.nn.relu(-traffic_gradients)
            return tf.reduce_mean(gradient_penalties)
        else:
            # è½¯çº¦æŸï¼šReLU(-E_batch[gk])
            batch_mean_gradient = tf.reduce_mean(traffic_gradients)
            return tf.nn.relu(-batch_mean_gradient)

    def train_step(self, data):
        """
        é«˜æ•ˆçš„è®­ç»ƒæ­¥éª¤ - å•æ¬¡å‰å‘ä¼ æ’­è§£å†³æ–¹æ¡ˆ + è¯¾ç¨‹å­¦ä¹ 
        
        å…³é”®ä¼˜åŒ–ï¼šä½¿ç”¨call_with_gradientsåœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­
        åŒæ—¶è·å¾—é¢„æµ‹å’Œæ¢¯åº¦ï¼Œæ¶ˆé™¤å†—ä½™è®¡ç®—ã€‚
        
        è¯¾ç¨‹å­¦ä¹ ï¼šåŠ¨æ€è°ƒæ•´ç‰©ç†çº¦æŸæƒé‡ï¼Œå®ç°å¹³æ»‘è®­ç»ƒè¿‡ç¨‹ã€‚
        """
        features, y_true = data
        
        # è·å–å½“å‰lambdaå€¼ï¼ˆè¯¾ç¨‹å­¦ä¹ æˆ–å›ºå®šå€¼ï¼‰
        current_lambda = self.current_lambda_physics if self.use_curriculum else self.lambda_physics
        
        with tf.GradientTape() as tape:
            # å…³é”®ï¼šå•æ¬¡å‰å‘ä¼ æ’­åŒæ—¶è·å¾—é¢„æµ‹å’Œæ¢¯åº¦
            predictions, traffic_gradients = self.call_with_gradients(features, training=True)
            
            # è®¡ç®—æ ‡å‡†æŸå¤±
            if self.target == 'delay':
                hetero_loss = self._compute_hetero_loss(y_true, predictions)
            else:  # drops
                hetero_loss = self._compute_binomial_loss(y_true, predictions)
            
            # è®¡ç®—æ¢¯åº¦çº¦æŸæŸå¤±
            if (self.use_physics_loss and self.target == 'delay' 
                and traffic_gradients is not None):
                gradient_loss = self._compute_gradient_loss_from_gradients(traffic_gradients)
                total_loss = hetero_loss + current_lambda * gradient_loss
            else:
                gradient_loss = tf.constant(0.0, dtype=tf.float32)
                total_loss = hetero_loss
            
            # æ·»åŠ æ­£åˆ™åŒ–æŸå¤±
            total_loss += sum(self.losses)
        
        # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
        gradients = tape.gradient(total_loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))
        
        # æ›´æ–°æŒ‡æ ‡
        self.total_loss_tracker.update_state(total_loss)
        self.hetero_loss_tracker.update_state(hetero_loss)
        self.gradient_loss_tracker.update_state(gradient_loss)
        
        # æ›´æ–°è¯¾ç¨‹å­¦ä¹ lambdaè¿½è¸ª
        if self.use_curriculum:
            self.lambda_tracker.update_state(current_lambda)
        
        result = {
            "total_loss": self.total_loss_tracker.result(),
            "hetero_loss": self.hetero_loss_tracker.result(), 
            "gradient_loss": self.gradient_loss_tracker.result(),
        }
        
        # æ·»åŠ lambdaè¿½è¸ªåˆ°è¿”å›ç»“æœ
        if self.use_curriculum:
            result["lambda_physics"] = self.lambda_tracker.result()
            
        return result

    def test_step(self, data):
        """æµ‹è¯•æ­¥éª¤"""
        features, y_true = data
        
        # å‰å‘ä¼ æ’­ï¼ˆæµ‹è¯•æ—¶ä¸éœ€è¦æ¢¯åº¦ï¼‰
        predictions = self(features, training=False)
        
        # è®¡ç®—æŸå¤±
        if self.target == 'delay':
            hetero_loss = self._compute_hetero_loss(y_true, predictions)
        else:
            hetero_loss = self._compute_binomial_loss(y_true, predictions)
        
        # æµ‹è¯•æ—¶é€šå¸¸ä¸è®¡ç®—æ¢¯åº¦çº¦æŸæŸå¤±ï¼Œä½†ä¸ºäº†ä¸€è‡´æ€§ä¿ç•™
        gradient_loss = tf.constant(0.0, dtype=tf.float32)
        total_loss = hetero_loss
        
        total_loss += sum(self.losses)
        
        # æ›´æ–°æŒ‡æ ‡
        self.total_loss_tracker.update_state(total_loss)
        self.hetero_loss_tracker.update_state(hetero_loss)
        self.gradient_loss_tracker.update_state(gradient_loss)
        
        return {
            "total_loss": self.total_loss_tracker.result(),
            "hetero_loss": self.hetero_loss_tracker.result(),
            "gradient_loss": self.gradient_loss_tracker.result(),
        }

# ==============================================================================
# 3. æŸå¤±å‡½æ•° (æ”¯æŒå»¶è¿Ÿå’Œä¸¢åŒ…ä¸¤ç§ä»»åŠ¡ + ç‰©ç†çº¦æŸ)
# ==============================================================================

def gradient_constraint_loss(model, features, predictions, use_hard_constraint=True):
    """
    æ¢¯åº¦çº¦æŸæŸå¤±å‡½æ•° - æ”¯æŒè½¯çº¦æŸå’Œç¡¬çº¦æŸ
    å¼ºåˆ¶æ¨¡å‹å­¦ä¹ ç¬¦åˆç‰©ç†ç›´è§‰çš„"æµé‡-å»¶è¿Ÿ"æ­£ç›¸å…³å…³ç³»
    
    Args:
        model: æ¨¡å‹å®ä¾‹
        features: è¾“å…¥ç‰¹å¾
        predictions: æ¨¡å‹é¢„æµ‹
        use_hard_constraint: Trueä¸ºç¡¬çº¦æŸ(é€æ ·æœ¬), Falseä¸ºè½¯çº¦æŸ(æ‰¹æ¬¡å¹³å‡)
    
    è½¯çº¦æŸå…¬å¼: L_gradient = ReLU(-E_batch[gk]) = max(0, -1/|batch| * sum(gk))
    ç¡¬çº¦æŸå…¬å¼: L_gradient = E_batch[ReLU(-gk)] = mean(max(0, -gk))
    """
    traffic = features['traffic']
    
    if predictions.shape[1] != 2:
        return tf.constant(0.0, dtype=tf.float32)
    
    with tf.GradientTape() as grad_tape:
        grad_tape.watch(traffic)
        pred_with_grad = model(features, training=True)
        loc = pred_with_grad[:, 0]
    
    gradients = grad_tape.gradient(loc, traffic)
    
    if gradients is None:
        return tf.constant(0.0, dtype=tf.float32)
    
    if use_hard_constraint:
        # ç¡¬çº¦æŸï¼šå…ˆå¯¹æ¯ä¸ªæ ·æœ¬çš„è´Ÿæ¢¯åº¦åº”ç”¨ReLUï¼Œå†æ±‚å¹³å‡å€¼
        # L_gradient = E_batch[ReLU(-gk)] = mean(max(0, -gk))
        gradient_penalties = tf.nn.relu(-gradients)
        return tf.reduce_mean(gradient_penalties)
    else:
        # è½¯çº¦æŸï¼šè®¡ç®—æ‰¹æ¬¡å†…çš„å¹³å‡æ¢¯åº¦ï¼Œç„¶ååº”ç”¨ReLU
        # L_gradient = ReLU(-E_batch[gk]) = max(0, -1/|batch| * sum(gk))
        batch_mean_gradient = tf.reduce_mean(gradients)
        gradient_penalty = tf.nn.relu(-batch_mean_gradient)
        return gradient_penalty

def heteroscedastic_loss(y_true, y_pred):
    """å¼‚æ–¹å·®æŸå¤±å‡½æ•° - ç”¨äºå»¶è¿Ÿé¢„æµ‹
    
    è¿™ä¸ªå®ç°ä¸åŸå§‹routenet.pyä¸­çš„å®ç°ä¿æŒä¸€è‡´ï¼š
    - ä½¿ç”¨ç›¸åŒçš„scaleè®¡ç®—æ–¹å¼ï¼ˆåŒ…å«cåç§»å¸¸æ•°ï¼‰
    - ä½¿ç”¨ç›¸åŒçš„_2sigmaè®¡ç®—æ–¹å¼
    """
    loc = y_pred[:, 0]
    
    # ä¸åŸç‰ˆä¿æŒä¸€è‡´çš„scaleè®¡ç®—ï¼ŒåŒ…å«é‡è¦çš„cåç§»å¸¸æ•°
    c = tf.math.log(tf.math.expm1(tf.constant(0.098, dtype=tf.float32)))
    scale = tf.nn.softplus(c + y_pred[:, 1]) + 1e-9
    
    delay_true = y_true['delay']
    jitter_true = y_true['jitter']
    packets_true = y_true['packets'] 
    drops_true = y_true['drops']
    
    n = packets_true - drops_true
    # ä¸åŸç‰ˆä¿æŒä¸€è‡´çš„_2sigmaè®¡ç®—
    _2sigma = tf.constant(2.0, dtype=tf.float32) * tf.square(scale)
    
    nll = (n * jitter_true / _2sigma + 
           n * tf.square(delay_true - loc) / _2sigma + 
           n * tf.math.log(scale))
           
    return tf.reduce_sum(nll) / 1e6

def binomial_loss(y_true, y_pred):
    """äºŒé¡¹åˆ†å¸ƒæŸå¤±å‡½æ•° - ç”¨äºä¸¢åŒ…é¢„æµ‹
    
    è¿™ä¸ªå®ç°ä¸åŸå§‹routenet.pyä¸­çš„å®ç°ä¿æŒä¸€è‡´ï¼š
    - ä½¿ç”¨ sigmoid_cross_entropy_with_logits æ¥è®¡ç®—äº¤å‰ç†µ
    - ä½¿ç”¨ packets ä½œä¸ºæƒé‡
    - ä½¿ç”¨ç›¸åŒçš„ç¼©æ”¾å› å­ 1e5
    """
    # y_pred æ˜¯ logitsï¼ˆæœªç»è¿‡ sigmoidï¼‰
    logits = y_pred[:, 0]
    
    packets_true = y_true['packets']
    drops_true = y_true['drops']
    
    # è®¡ç®—çœŸå®ä¸¢åŒ…ç‡ (è¿™æ˜¯æ ‡ç­¾)
    loss_ratio = drops_true / (packets_true + 1e-9)
    
    # ä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„å…¬å¼ï¼š
    # tf.reduce_sum(packets * sigmoid_cross_entropy_with_logits(labels=loss_ratio, logits=logits)) / 1e5
    loss = tf.reduce_sum(
        packets_true * tf.nn.sigmoid_cross_entropy_with_logits(
            labels=loss_ratio,
            logits=logits
        )
    ) / 1e5  # ä½¿ç”¨ä¸åŸç‰ˆç›¸åŒçš„ç¼©æ”¾å› å­
    
    return loss

def physics_informed_loss(y_true, y_pred, model, features, lambda_physics=0.1, use_hard_constraint=True):
    """
    ç‰©ç†çº¦æŸæŸå¤±å‡½æ•° (Physics-Informed Loss Function)
    
    æ€»æŸå¤±å‡½æ•°: L_total = L_hetero + Î» * L_gradient
    - L_hetero: å¼‚æ–¹å·®æŸå¤±å‡½æ•°ï¼ˆæ•°æ®æ‹Ÿåˆé¡¹ï¼‰
    - L_gradient: æ¢¯åº¦çº¦æŸæŸå¤±ï¼ˆç‰©ç†çº¦æŸé¡¹ï¼‰
    - Î»: å¹³è¡¡è¶…å‚æ•°
    
    Args:
        y_true: çœŸå®æ ‡ç­¾
        y_pred: æ¨¡å‹é¢„æµ‹
        model: æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºè®¡ç®—æ¢¯åº¦ï¼‰
        features: è¾“å…¥ç‰¹å¾ï¼ˆåŒ…å«trafficï¼‰
        lambda_physics: ç‰©ç†çº¦æŸæƒé‡ç³»æ•°
        use_hard_constraint: Trueä¸ºç¡¬çº¦æŸï¼ŒFalseä¸ºè½¯çº¦æŸ
    """
    # æ•°æ®æ‹Ÿåˆé¡¹ï¼šå¼‚æ–¹å·®æŸå¤±
    l_hetero = heteroscedastic_loss(y_true, y_pred)
    
    # ç‰©ç†çº¦æŸé¡¹ï¼šæ¢¯åº¦çº¦æŸæŸå¤±
    l_gradient = gradient_constraint_loss(model, features, y_pred, use_hard_constraint)
    
    # æ€»æŸå¤±
    l_total = l_hetero + lambda_physics * l_gradient
    
    return l_total, l_hetero, l_gradient

def create_model_and_loss_fn(config, target, use_kan=False, use_physics_loss=False, use_hard_constraint=True, 
                           lambda_physics=0.1, use_optimized_model=True, use_curriculum=False, 
                           warmup_epochs=5, ramp_epochs=10, max_lambda=1.0):
    """æ ¹æ®targetå‚æ•°åˆ›å»ºç›¸åº”çš„æ¨¡å‹å’ŒæŸå¤±å‡½æ•°
    
    Args:
        config: æ¨¡å‹é…ç½®
        target: é¢„æµ‹ç›®æ ‡ ('delay' æˆ– 'drops')
        use_kan: æ˜¯å¦ä½¿ç”¨KANæ¶æ„
        use_physics_loss: æ˜¯å¦ä½¿ç”¨ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°
        use_hard_constraint: Trueä¸ºç¡¬çº¦æŸ(é€æ ·æœ¬)ï¼ŒFalseä¸ºè½¯çº¦æŸ(æ‰¹æ¬¡å¹³å‡)
        lambda_physics: ç‰©ç†çº¦æŸæƒé‡ç³»æ•°ï¼ˆå›ºå®šå€¼æˆ–è¯¾ç¨‹å­¦ä¹ çš„æœ€å¤§å€¼ï¼‰
        use_optimized_model: Trueä½¿ç”¨é«˜æ•ˆç‰©ç†çº¦æŸæ¨¡å‹ï¼ŒFalseä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹
        use_curriculum: æ˜¯å¦å¯ç”¨è¯¾ç¨‹å­¦ä¹ 
        warmup_epochs: çƒ­èº«æœŸè½®æ•°
        ramp_epochs: å¢é•¿æœŸè½®æ•°
        max_lambda: è¯¾ç¨‹å­¦ä¹ çš„æœ€å¤§lambdaå€¼
    """
    model_type = "KAN-based" if use_kan else "MLP-based"
    
    # ç¡®å®šçº¦æŸç±»å‹
    if use_physics_loss:
        constraint_type = "hard-constraint" if use_hard_constraint else "soft-constraint"
    else:
        constraint_type = "standard"
    
    if use_optimized_model and use_physics_loss:
        # ä½¿ç”¨é«˜æ•ˆçš„ç‰©ç†çº¦æŸæ¨¡å‹ - è§£å†³å†—ä½™å‰å‘ä¼ æ’­é—®é¢˜
        model = PhysicsInformedRouteNet(
            config=config,
            target=target,
            use_kan=use_kan,
            use_physics_loss=use_physics_loss,
            use_hard_constraint=use_hard_constraint,
            lambda_physics=lambda_physics,
            use_curriculum=use_curriculum,
            warmup_epochs=warmup_epochs,
            ramp_epochs=ramp_epochs,
            max_lambda=max_lambda
        )
        
        # é«˜æ•ˆæ¨¡å‹ä¸éœ€è¦å•ç‹¬çš„æŸå¤±å‡½æ•°ï¼ŒæŸå¤±è®¡ç®—é›†æˆåœ¨train_stepä¸­
        loss_fn = None  
        
        print(f"ğŸš€ Created OPTIMIZED {model_type} {target} prediction model")
        print(f"   - Physics Loss: {use_physics_loss}")
        print(f"   - Constraint Type: {constraint_type}")
        if use_curriculum:
            print(f"   - Curriculum Learning: Enabled (max_lambda={max_lambda})")
        else:
            print(f"   - Lambda Physics: {lambda_physics}")
        print(f"   - Performance: Single forward pass (2x speed improvement)")
        
    else:
        # ä½¿ç”¨ä¼ ç»Ÿæ¨¡å‹ï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰
        if target == 'delay':
            # å»¶è¿Ÿé¢„æµ‹æ¨¡å‹
            model = RouteNet(config, output_units=2, final_activation=None, use_kan=use_kan)
            
            if use_physics_loss:
                # ä½¿ç”¨ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°ï¼ˆä¼ ç»Ÿæ–¹å¼ - æœ‰å†—ä½™å‰å‘ä¼ æ’­ï¼‰
                def loss_fn(labels, predictions, model=model, features=None):
                    if features is None:
                        return heteroscedastic_loss(labels, predictions)
                    return physics_informed_loss(labels, predictions, model, features, lambda_physics, use_hard_constraint)
                print("âš ï¸ Created TRADITIONAL {} delay prediction model with {} (Î»={})".format(
                    model_type, constraint_type, lambda_physics))
                print("   - Warning: This uses double forward pass (slower)")
            else:
                # ä½¿ç”¨æ ‡å‡†å¼‚æ–¹å·®æŸå¤±
                loss_fn = heteroscedastic_loss
                print("Created {} delay prediction model with {} loss".format(
                    model_type, constraint_type))
                
        elif target == 'drops':
            # ä¸¢åŒ…é¢„æµ‹æ¨¡å‹
            model = RouteNet(config, output_units=1, final_activation=None, use_kan=use_kan)
            loss_fn = binomial_loss
            if use_physics_loss:
                print("Warning: Physics constraints are not implemented for drops prediction. Using standard binomial loss.")
            print("Created {} drop prediction model with binomial loss".format(model_type))
        else:
            raise ValueError("Unsupported target: {}. Choose 'delay' or 'drops'".format(target))
    
    return model, loss_fn

@tf.function
def train_step(model, optimizer, features, labels, loss_fn, use_physics_loss=False):
    """è®­ç»ƒæ­¥éª¤ - æ”¯æŒç‰©ç†çº¦æŸæŸå¤±"""
    with tf.GradientTape() as tape:
        predictions = model(features, training=True)
        
        if use_physics_loss:
            # ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°éœ€è¦é¢å¤–å‚æ•°
            if hasattr(loss_fn, '__call__'):
                try:
                    # å°è¯•è°ƒç”¨ç‰©ç†çº¦æŸæŸå¤±å‡½æ•°
                    loss_result = loss_fn(labels, predictions, model, features)
                    if isinstance(loss_result, tuple):
                        # è¿”å› (total_loss, hetero_loss, gradient_loss)
                        loss, l_hetero, l_gradient = loss_result
                    else:
                        loss = loss_result
                        l_hetero = tf.constant(0.0)
                        l_gradient = tf.constant(0.0)
                except:
                    # é€€å›åˆ°æ ‡å‡†æŸå¤±
                    loss = loss_fn(labels, predictions)
                    l_hetero = loss
                    l_gradient = tf.constant(0.0)
            else:
                loss = loss_fn(labels, predictions)
                l_hetero = loss
                l_gradient = tf.constant(0.0)
        else:
            # æ ‡å‡†æŸå¤±å‡½æ•°
            loss = loss_fn(labels, predictions)
            l_hetero = loss
            l_gradient = tf.constant(0.0)
            
        # æ·»åŠ æ¨¡å‹æ­£åˆ™åŒ–æŸå¤±
        loss += sum(model.losses)
        
    gradients = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    
    return loss, predictions, l_hetero, l_gradient

@tf.function  
def eval_step(model, features, labels, loss_fn):
    predictions = model(features, training=False)
    loss = loss_fn(labels, predictions)
    loss += sum(model.losses)
    return loss, predictions

# ==============================================================================
# 4. ä¸»æ‰§è¡Œé€»è¾‘
# ==============================================================================

def main(args):
    # =============================
    # 0. è®¾å®šå…¨å±€éšæœºç§å­ (å¤ç°æ€§)
    # =============================
    def set_global_determinism(seed: int):
        os.environ['PYTHONHASHSEED'] = str(seed)
        os.environ['TF_DETERMINISTIC_OPS'] = '1'  # å°è¯•ä½¿ç”¨ç¡®å®šæ€§ç®—å­ï¼ˆè‹¥å¯ç”¨ï¼‰
        random.seed(seed)
        np.random.seed(seed)
        tf.random.set_seed(seed)
        print(f"[Seed] Global random seed set to {seed}")

    set_global_determinism(args.seed)

    config = {
        'link_state_dim': 4,
        'path_state_dim': 2, 
        'T': 3,
        'readout_units': 8,
        'readout_layers': 2,
        'l2': 0.1,
        'l2_2': 0.01,
        # KAN å‚æ•°é»˜è®¤å€¼ï¼ˆå½“ä½¿ç”¨KANæ—¶ç”Ÿæ•ˆï¼‰
        'kan_basis': 'poly',        # 'poly' æˆ– 'bspline'
        'kan_grid_size': 5,         # Bæ ·æ¡é—´éš”æ•°ï¼Œåªæœ‰åœ¨bsplineæ—¶ä½¿ç”¨
        'kan_spline_order': 3,      # Bæ ·æ¡é˜¶æ¬¡ï¼ˆdegreeï¼‰ï¼Œå…¸å‹ä¸º3
        'dropout_rate': args.dropout_rate,
    }

    # è®¾ç½® TensorBoard æ—¥å¿—ç›®å½•ï¼ŒåŒ…å«targetå’ŒKANä¿¡æ¯
    current_time = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    model_type = "kan" if args.kan else "mlp"
    log_dir = os.path.join(args.model_dir, 'logs', '{}_{}_{}'.format(args.target, model_type, current_time))
    train_log_dir = os.path.join(log_dir, 'train')
    val_log_dir = os.path.join(log_dir, 'validation')
    
    # åˆ›å»º TensorBoard å†™å…¥å™¨
    train_summary_writer = tf.summary.create_file_writer(train_log_dir)
    val_summary_writer = tf.summary.create_file_writer(val_log_dir)

    # --------------------------------------------------
    # HParams é…ç½®ä¸è®°å½• (TensorBoard HParams Plugin)
    # --------------------------------------------------
    # å£°æ˜å¯è§†åŒ–çš„è¶…å‚æ•°ä¸æŒ‡æ ‡ (æ¯ä¸ª run ç‹¬ç«‹ç›®å½•ï¼Œä¸ä¼šé‡å¤å†²çª)
    hp.hparams_config(
        hparams=[
            hp.HParam('seed', hp.IntInterval(0, 10**9)),
            hp.HParam('target', hp.Discrete(['delay', 'drops'])),
            hp.HParam('kan', hp.Discrete([0, 1])),
            hp.HParam('kan_basis', hp.Discrete(['poly', 'bspline'])),
            hp.HParam('kan_grid_size', hp.IntInterval(1, 64)),
            hp.HParam('kan_spline_order', hp.IntInterval(1, 10)),
            hp.HParam('learning_rate', hp.RealInterval(1e-6, 1.0)),
            hp.HParam('lr_schedule', hp.Discrete(['fixed','exponential','cosine','polynomial','plateau'])),
            hp.HParam('batch_size', hp.IntInterval(1, 65536)),
            hp.HParam('physics_loss', hp.Discrete([0,1])),
            hp.HParam('hard_physics', hp.Discrete([0,1])),
            hp.HParam('lambda_physics', hp.RealInterval(0.0, 10.0)),
            hp.HParam('curriculum', hp.Discrete([0,1])),
            hp.HParam('warmup_epochs', hp.IntInterval(0, 1000)),
            hp.HParam('ramp_epochs', hp.IntInterval(0, 1000)),
            hp.HParam('max_lambda', hp.RealInterval(0.0, 10.0)),
            hp.HParam('T', hp.IntInterval(1, 32)),
            hp.HParam('readout_layers', hp.IntInterval(1, 32)),
            hp.HParam('readout_units', hp.IntInterval(1, 1024)),
            hp.HParam('dropout_rate', hp.RealInterval(0.0, 1.0)),
        ],
        metrics=[
            hp.Metric('final_best_eval_loss', display_name='Best Val Loss'),
        ]
    )

    # å½“å‰è¿è¡Œçš„è¶…å‚æ•°å­—å…¸
    hparams_run = {
        'seed': args.seed,
        'target': args.target,
        'kan': int(args.kan),
        'kan_basis': args.kan_basis or 'poly',
        'kan_grid_size': args.kan_grid_size if args.kan_grid_size is not None else config['kan_grid_size'],
        'kan_spline_order': args.kan_spline_order if args.kan_spline_order is not None else config['kan_spline_order'],
        'learning_rate': args.learning_rate,
        'lr_schedule': args.lr_schedule,
        'batch_size': args.batch_size,
        'physics_loss': int(args.physics_loss),
        'hard_physics': int(args.hard_physics),
        'lambda_physics': args.lambda_physics,
        'curriculum': int(args.curriculum),
        'warmup_epochs': args.warmup_epochs,
        'ramp_epochs': args.ramp_epochs,
        'max_lambda': args.max_lambda,
        'T': 3,  # å›ºå®šåœ¨configä¸­
        'readout_layers': config['readout_layers'],
        'readout_units': config['readout_units'],
        'dropout_rate': config.get('dropout_rate', 0.1),
    }

    # å†™å…¥ hparams ï¼ˆéœ€åœ¨è®­ç»ƒå¼€å§‹å‰å†™å…¥ä¸€æ¬¡ï¼‰
    with train_summary_writer.as_default():
        hp.hparams(hparams_run)

    train_files = tf.io.gfile.glob(os.path.join(args.train_dir, '*.tfrecords'))
    train_dataset = create_dataset(train_files, args.batch_size, is_training=True)
    print("Found {} training files.".format(len(train_files)))

    eval_files = tf.io.gfile.glob(os.path.join(args.eval_dir, '*.tfrecords'))
    eval_dataset = create_dataset(eval_files, args.batch_size, is_training=False)
    print("Found {} evaluation files.".format(len(eval_files)))

    # æ ¹æ®targetåˆ›å»ºæ¨¡å‹å’ŒæŸå¤±å‡½æ•°
    # å°† CLI çš„ KAN å‚æ•°å†™å…¥ configï¼ˆä»…å½“ç”¨æˆ·ä¼ å…¥æ—¶è¦†ç›–é»˜è®¤å€¼ï¼‰
    if args.kan_basis is not None:
        config['kan_basis'] = args.kan_basis
    if args.kan_grid_size is not None:
        config['kan_grid_size'] = args.kan_grid_size
    if args.kan_spline_order is not None:
        config['kan_spline_order'] = args.kan_spline_order

    model, loss_fn = create_model_and_loss_fn(config, args.target, 
                                             use_kan=args.kan, 
                                             use_physics_loss=args.physics_loss,
                                             use_hard_constraint=args.hard_physics,
                                             lambda_physics=args.lambda_physics,
                                             use_optimized_model=True,  # é»˜è®¤ä½¿ç”¨ä¼˜åŒ–æ¨¡å‹
                                             use_curriculum=args.curriculum,
                                             warmup_epochs=args.warmup_epochs,
                                             ramp_epochs=args.ramp_epochs,
                                             max_lambda=args.max_lambda)
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨é«˜æ•ˆæ¨¡å‹
    use_optimized_training = isinstance(model, PhysicsInformedRouteNet)
    
    # åˆ›å»ºåŠ¨æ€å­¦ä¹ ç‡è°ƒåº¦å™¨
    if args.lr_schedule == 'exponential':
        lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
            initial_learning_rate=args.learning_rate,
            decay_steps=args.decay_steps,
            decay_rate=args.decay_rate,
            staircase=True
        )
        print("Using exponential decay learning rate schedule")
    elif args.lr_schedule == 'cosine':
        lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
            initial_learning_rate=args.learning_rate,
            decay_steps=args.epochs * args.steps_per_epoch,
            alpha=0.01  # æœ€å°å­¦ä¹ ç‡æ˜¯åˆå§‹å­¦ä¹ ç‡çš„1%
        )
        print("Using cosine decay learning rate schedule")
    elif args.lr_schedule == 'polynomial':
        lr_schedule = tf.keras.optimizers.schedules.PolynomialDecay(
            initial_learning_rate=args.learning_rate,
            decay_steps=args.epochs * args.steps_per_epoch,
            end_learning_rate=args.learning_rate * 0.01,
            power=1.0
        )
        print("Using polynomial decay learning rate schedule")
    elif args.lr_schedule == 'plateau':
        # ç”¨äºReduceLROnPlateauçš„åˆå§‹å­¦ä¹ ç‡
        lr_schedule = args.learning_rate
        print("Using ReduceLROnPlateau learning rate schedule")
    else:
        # å›ºå®šå­¦ä¹ ç‡
        lr_schedule = args.learning_rate
        print("Using fixed learning rate: {}".format(args.learning_rate))
    
    optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)
    
    # å¦‚æœä½¿ç”¨é«˜æ•ˆæ¨¡å‹ï¼Œéœ€è¦ç¼–è¯‘æ¨¡å‹å¹¶è®¾ç½®ä¼˜åŒ–å™¨
    if use_optimized_training:
        model.compile(optimizer=optimizer)
        print("âœ… Compiled optimized model with Adam optimizer")
    
    # å¦‚æœä½¿ç”¨plateauè°ƒåº¦ï¼Œåˆ›å»ºReduceLROnPlateau callback
    reduce_lr_callback = None
    if args.lr_schedule == 'plateau':
        reduce_lr_callback = tf.keras.callbacks.ReduceLROnPlateau(
            monitor='val_loss',
            factor=args.plateau_factor,
            patience=args.plateau_patience,
            min_lr=args.learning_rate * 0.001,
            verbose=1
        )
    
    print("Starting training for target: {}".format(args.target))
    print("TensorBoard logs will be saved to: {}".format(log_dir))
    print("Run 'tensorboard --logdir {}' to view training progress".format(log_dir))
    
    best_eval_loss = float('inf')
    global_step = 0

    # Helper to safely convert Tensor/NumPy scalars to Python float
    def _to_float(x):
        try:
            import numpy as _np  # local import to avoid global dependency issues
            if isinstance(x, (float, int)):
                return float(x)
            if hasattr(x, 'numpy'):
                return float(x.numpy())
            if isinstance(x, _np.ndarray) and x.shape == ():
                return float(x.item())
            return float(x)
        except Exception:
            # Fallback: do not raise inside training loop; return NaN to surface issue in logs
            return float('nan')
    
    # æ—©åœå˜é‡åˆå§‹åŒ–
    if args.early_stopping:
        early_stopping_patience = args.early_stopping_patience
        early_stopping_counter = 0
        early_stopping_min_delta = args.early_stopping_min_delta
        best_weights = None
        print(f"ğŸ›‘ Early stopping enabled: patience={early_stopping_patience}, min_delta={early_stopping_min_delta}")
    else:
        early_stopping_min_delta = 0.0  # ä¸ºéæ—©åœæ¨¡å¼è®¾ç½®é»˜è®¤å€¼
    
    for epoch in range(args.epochs):
        print("\nEpoch {}/{}".format(epoch + 1, args.epochs))
        
        # è¯¾ç¨‹å­¦ä¹ ï¼šåœ¨æ¯ä¸ªepochå¼€å§‹æ—¶æ›´æ–°lambda_physics
        if use_optimized_training and hasattr(model, 'use_curriculum') and model.use_curriculum:
            new_lambda = model.update_curriculum_lambda(epoch)
            current_lambda = model.get_current_lambda()
            
            # ç¡®å®šå½“å‰è®­ç»ƒé˜¶æ®µ
            if epoch < model.warmup_epochs:
                stage = "Warmup"
            elif epoch < model.warmup_epochs + model.ramp_epochs:
                stage = "Ramp-up"
                progress = (epoch - model.warmup_epochs) / model.ramp_epochs * 100
                stage += f" ({progress:.1f}%)"
            else:
                stage = "Hold"
                
            print(f"ğŸ“š Curriculum Learning - Epoch {epoch+1}: Î»={current_lambda:.4f} [{stage}]")
        
        # è®­ç»ƒ
        total_train_loss = 0.0
        total_hetero_loss = 0.0
        total_gradient_loss = 0.0
        total_lambda_physics = 0.0
        train_step_count = 0
        pbar = tqdm(train_dataset, desc="Training Epoch {}".format(epoch+1))
        
        for features, labels in pbar:
            if use_optimized_training:
                # ä½¿ç”¨é«˜æ•ˆæ¨¡å‹çš„å†…ç½®train_step
                metrics = model.train_step((features, labels))
                loss = metrics["total_loss"]
                l_hetero = metrics["hetero_loss"] 
                l_gradient = metrics["gradient_loss"]
            else:
                # ä½¿ç”¨ä¼ ç»Ÿè®­ç»ƒæ­¥éª¤
                loss, predictions, l_hetero, l_gradient = train_step(model, optimizer, features, labels, loss_fn, use_physics_loss=args.physics_loss)
            
            total_train_loss += loss
            total_hetero_loss += l_hetero
            total_gradient_loss += l_gradient
            
            # è¯¾ç¨‹å­¦ä¹ lambdaè¿½è¸ª
            if use_optimized_training and "lambda_physics" in metrics:
                total_lambda_physics += metrics["lambda_physics"]
            
            train_step_count += 1
            global_step += 1
            
            # è®°å½•æ¯ä¸ªæ‰¹æ¬¡çš„æŸå¤±åˆ° TensorBoard (æ¯10æ­¥è®°å½•ä¸€æ¬¡)
            if global_step % 10 == 0:
                with train_summary_writer.as_default():
                    tf.summary.scalar('batch_loss', loss, step=global_step)
                    if args.physics_loss:
                        tf.summary.scalar('batch_hetero_loss', l_hetero, step=global_step)
                        tf.summary.scalar('batch_gradient_loss', l_gradient, step=global_step)
                        # è®°å½•è¯¾ç¨‹å­¦ä¹ lambdaå€¼
                        if use_optimized_training and hasattr(model, 'use_curriculum') and model.use_curriculum:
                            tf.summary.scalar('lambda_physics', model.get_current_lambda(), step=global_step)
                    # è®°å½•å½“å‰å­¦ä¹ ç‡
                    current_lr = optimizer.learning_rate
                    if hasattr(current_lr, 'numpy'):
                        current_lr = current_lr.numpy()
                    tf.summary.scalar('learning_rate', current_lr, step=global_step)
            
            # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
            if args.physics_loss:
                pbar.set_postfix({
                    'total': '{:.4f}'.format(loss), 
                    'hetero': '{:.4f}'.format(l_hetero),
                    'grad': '{:.4f}'.format(l_gradient),
                    'step': global_step
                })
            else:
                pbar.set_postfix({'loss': '{:.4f}'.format(loss), 'step': global_step})
        
        avg_train_loss = total_train_loss / train_step_count
        avg_hetero_loss = total_hetero_loss / train_step_count 
        avg_gradient_loss = total_gradient_loss / train_step_count
        avg_lambda_physics = total_lambda_physics / train_step_count if total_lambda_physics > 0 else 0

        # Ensure averages are Python floats (avoid Tensor <-> None comparisons later)
        avg_train_loss = _to_float(avg_train_loss)
        avg_hetero_loss = _to_float(avg_hetero_loss)
        avg_gradient_loss = _to_float(avg_gradient_loss)
        avg_lambda_physics = _to_float(avg_lambda_physics)

        # è®°å½•è®­ç»ƒçš„å¹³å‡æŸå¤±
        with train_summary_writer.as_default():
            tf.summary.scalar('epoch_loss', avg_train_loss, step=epoch + 1)
            if args.physics_loss:
                tf.summary.scalar('epoch_hetero_loss', avg_hetero_loss, step=epoch + 1)
                tf.summary.scalar('epoch_gradient_loss', avg_gradient_loss, step=epoch + 1)
                # è®°å½•è¯¾ç¨‹å­¦ä¹ çš„epochçº§lambdaå€¼
                if use_optimized_training and hasattr(model, 'use_curriculum') and model.use_curriculum:
                    tf.summary.scalar('epoch_lambda_physics', model.get_current_lambda(), step=epoch + 1)

        # è¯„ä¼°
        total_eval_loss = 0.0
        eval_step_count = 0
        pbar_eval = tqdm(eval_dataset, desc="Evaluating Epoch {}".format(epoch+1))
        
        for features, labels in pbar_eval:
            if use_optimized_training:
                # ä½¿ç”¨é«˜æ•ˆæ¨¡å‹çš„å†…ç½®test_step
                metrics = model.test_step((features, labels))
                loss = metrics["total_loss"]
            else:
                # ä½¿ç”¨ä¼ ç»Ÿè¯„ä¼°æ­¥éª¤
                loss, predictions = eval_step(model, features, labels, loss_fn)
            
            total_eval_loss += loss
            eval_step_count += 1
            pbar_eval.set_postfix({'loss': '{:.4f}'.format(loss)})
        
        avg_eval_loss = total_eval_loss / eval_step_count
        avg_eval_loss = _to_float(avg_eval_loss)

        # è®°å½•éªŒè¯æŸå¤±
        with val_summary_writer.as_default():
            tf.summary.scalar('epoch_loss', avg_eval_loss, step=epoch + 1)
            # è®°å½•æ¯ä¸ªepochç»“æŸæ—¶çš„å­¦ä¹ ç‡
            current_lr = optimizer.learning_rate
            if hasattr(current_lr, 'numpy'):
                current_lr = current_lr.numpy()
            tf.summary.scalar('learning_rate_epoch', current_lr, step=epoch + 1)

        # å¦‚æœä½¿ç”¨ReduceLROnPlateauï¼Œæ‰‹åŠ¨è°ƒæ•´å­¦ä¹ ç‡
        if reduce_lr_callback is not None:
            # Simulate ReduceLROnPlateau manually while ensuring we work with pure floats
            current_loss = _to_float(avg_eval_loss)
            if not hasattr(reduce_lr_callback, 'best') or reduce_lr_callback.best is None:
                reduce_lr_callback.best = current_loss
                reduce_lr_callback.wait = 0
            else:
                if current_loss < reduce_lr_callback.best:
                    reduce_lr_callback.best = current_loss
                    reduce_lr_callback.wait = 0
                else:
                    reduce_lr_callback.wait += 1
                    if reduce_lr_callback.wait >= args.plateau_patience:
                        old_lr = optimizer.learning_rate.numpy() if hasattr(optimizer.learning_rate, 'numpy') else float(optimizer.learning_rate)
                        new_lr = old_lr * args.plateau_factor
                        if new_lr >= args.learning_rate * 0.001:
                            if hasattr(optimizer.learning_rate, 'assign'):
                                optimizer.learning_rate.assign(new_lr)
                            else:  # fallback if lr is plain float (unlikely with Adam)
                                optimizer.learning_rate = new_lr
                            print("Reducing learning rate from {:.6f} to {:.6f}".format(old_lr, new_lr))
                            reduce_lr_callback.wait = 0

        # è¾“å‡ºepochç»“æœ
        if args.physics_loss:
            lr_value = optimizer.learning_rate.numpy() if hasattr(optimizer.learning_rate, 'numpy') else optimizer.learning_rate
            if use_optimized_training and hasattr(model, 'use_curriculum') and model.use_curriculum:
                print("Epoch {} finished. Total Loss: {:.4f} (Hetero: {:.4f}, Gradient: {:.4f}), Eval Loss: {:.4f}, LR: {:.6f}, Î»: {:.4f}".format(
                    epoch + 1, avg_train_loss, avg_hetero_loss, avg_gradient_loss, avg_eval_loss, lr_value, model.get_current_lambda()))
            else:
                print("Epoch {} finished. Total Loss: {:.4f} (Hetero: {:.4f}, Gradient: {:.4f}), Eval Loss: {:.4f}, LR: {:.6f}".format(
                    epoch + 1, avg_train_loss, avg_hetero_loss, avg_gradient_loss, avg_eval_loss, lr_value))
        else:
            print("Epoch {} finished. Avg Train Loss: {:.4f}, Avg Eval Loss: {:.4f}, LR: {:.6f}".format(
                epoch + 1, avg_train_loss, avg_eval_loss, 
                optimizer.learning_rate.numpy() if hasattr(optimizer.learning_rate, 'numpy') else optimizer.learning_rate))

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_eval_loss < best_eval_loss - early_stopping_min_delta if args.early_stopping else avg_eval_loss < best_eval_loss:
            print("Evaluation loss improved from {:.4f} to {:.4f}. Saving model...".format(
                best_eval_loss, avg_eval_loss))
            best_eval_loss = avg_eval_loss
            
            # æ ¹æ®æ˜¯å¦ä½¿ç”¨KANæ¥å‘½åæ¨¡å‹æ–‡ä»¶
            model_suffix = "kan_model" if args.kan else "model"
            save_path = os.path.join(args.model_dir, "best_{}_{}.weights.h5".format(args.target, model_suffix))
            model.save_weights(save_path)
            
            # æ—©åœï¼šé‡ç½®è®¡æ•°å™¨å¹¶ä¿å­˜æœ€ä½³æƒé‡
            if args.early_stopping:
                early_stopping_counter = 0
                if args.early_stopping_restore_best:
                    best_weights = model.get_weights()
                    print("ğŸ”„ Early stopping: best weights saved")
            
            # è®°å½•æœ€ä½³æ¨¡å‹çš„ä¿¡æ¯
            with val_summary_writer.as_default():
                tf.summary.scalar('best_loss', best_eval_loss, step=epoch + 1)
        else:
            print("Evaluation loss did not improve from {:.4f}.".format(best_eval_loss))
            
            # æ—©åœï¼šå¢åŠ è®¡æ•°å™¨
            if args.early_stopping:
                early_stopping_counter += 1
                print(f"ğŸ›‘ Early stopping: {early_stopping_counter}/{early_stopping_patience}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦æ—©åœ
                if early_stopping_counter >= early_stopping_patience:
                    print(f"ğŸ›‘ Early stopping triggered after {epoch + 1} epochs!")
                    print(f"ğŸ›‘ No improvement for {early_stopping_patience} consecutive epochs")
                    
                    # æ¢å¤æœ€ä½³æƒé‡
                    if args.early_stopping_restore_best and best_weights is not None:
                        model.set_weights(best_weights)
                        print("ğŸ”„ Restored best model weights")
                    
                    break
    
    # è®­ç»ƒç»“æŸåï¼ˆæ—©åœæˆ–æ­£å¸¸å®Œæˆï¼‰ï¼Œè®°å½•æœ€ç»ˆæœ€ä½³éªŒè¯æŸå¤±åˆ° HParams æ‰€åœ¨ writer
    with train_summary_writer.as_default():
        tf.summary.scalar('final_best_eval_loss', best_eval_loss, step=0)

    # è®­ç»ƒç»“æŸåï¼Œå…³é—­ summary writers
    train_summary_writer.close()
    val_summary_writer.close()
    
    # è®­ç»ƒå®Œæˆç»Ÿè®¡
    if args.early_stopping:
        if early_stopping_counter >= early_stopping_patience:
            print(f"\nğŸ›‘ Training stopped early after {epoch + 1}/{args.epochs} epochs")
            print(f"ğŸ›‘ Best validation loss: {best_eval_loss:.6f}")
        else:
            print(f"\nâœ… Training completed normally after {args.epochs} epochs")
            print(f"âœ… Best validation loss: {best_eval_loss:.6f}")
    else:
        print(f"\nâœ… Training completed after {args.epochs} epochs")
        print(f"âœ… Best validation loss: {best_eval_loss:.6f}")
    
    print("TensorBoard logs saved to: {}".format(log_dir))
    print("To view the results, run: tensorboard --logdir {}".format(log_dir))
    
    # æ ¹æ®æ˜¯å¦ä½¿ç”¨KANæ¥æ˜¾ç¤ºæ¨¡å‹æ–‡ä»¶å
    model_suffix = "kan_model" if args.kan else "model"
    print("Model weights saved as: {}".format(
        os.path.join(args.model_dir, "best_{}_{}.weights.h5".format(args.target, model_suffix))))

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='RouteNet TF2 Implementation')
    parser.add_argument('--train_dir', type=str, required=True, 
                      help='Directory containing training TFRecord files')
    parser.add_argument('--eval_dir', type=str, required=True,
                      help='Directory containing evaluation TFRecord files') 
    parser.add_argument('--model_dir', type=str, required=True,
                      help='Directory to save model checkpoints and logs')
    parser.add_argument('--target', type=str, choices=['delay', 'drops'], default='delay',
                      help='Training target: "delay" for delay prediction, "drops" for packet drop prediction')
    parser.add_argument('--kan', action='store_true', 
                      help='Use KAN (Kolmogorov-Arnold Networks) instead of traditional MLP for readout layers')
    # KAN å‚æ•°
    parser.add_argument('--kan_basis', type=str, choices=['poly', 'bspline'], default=None,
                      help='KAN basis type for readout: poly (default) or bspline')
    parser.add_argument('--kan_grid_size', type=int, default=None,
                      help='Number of intervals for B-spline grid (only for bspline basis)')
    parser.add_argument('--kan_spline_order', type=int, default=None,
                      help='Degree/order of B-spline basis (only for bspline basis)')
    parser.add_argument('--epochs', type=int, default=10,
                      help='Number of training epochs')
    parser.add_argument('--batch_size', type=int, default=32,
                      help='Training batch size')
    parser.add_argument('--learning_rate', type=float, default=0.001,
                      help='Initial learning rate for Adam optimizer')
    parser.add_argument('--lr_schedule', type=str, choices=['fixed', 'exponential', 'cosine', 'polynomial', 'plateau'], 
                      default='fixed', help='Learning rate schedule strategy')
    
    # æŒ‡æ•°è¡°å‡å‚æ•°
    parser.add_argument('--decay_steps', type=int, default=1000,
                      help='Steps for exponential decay (only for exponential schedule)')
    parser.add_argument('--decay_rate', type=float, default=0.96,
                      help='Decay rate for exponential decay (only for exponential schedule)')
    
    # Plateauè°ƒåº¦å‚æ•°
    parser.add_argument('--plateau_factor', type=float, default=0.5,
                      help='Factor to reduce learning rate on plateau (only for plateau schedule)')
    parser.add_argument('--plateau_patience', type=int, default=3,
                      help='Number of epochs to wait before reducing LR on plateau (only for plateau schedule)')
    
    # æ—©åœå‚æ•°
    parser.add_argument('--early_stopping', action='store_true',
                      help='Enable early stopping based on validation loss')
    parser.add_argument('--early_stopping_patience', type=int, default=5,
                      help='Number of epochs to wait before early stopping (default: 5)')
    parser.add_argument('--early_stopping_min_delta', type=float, default=1e-6,
                      help='Minimum change in monitored quantity to qualify as an improvement (default: 1e-6)')
    parser.add_argument('--early_stopping_restore_best', action='store_true',
                      help='Restore model weights from the epoch with the best value of the monitored quantity')
    
    # ç‰©ç†çº¦æŸæŸå¤±å‚æ•°
    parser.add_argument('--physics_loss', action='store_true',
                      help='Enable physics-informed loss function for delay prediction')
    parser.add_argument('--hard_physics', action='store_true',
                      help='Use hard constraint (per-sample) instead of soft constraint (batch-average). Only effective when --physics_loss is enabled.')
    parser.add_argument('--lambda_physics', type=float, default=0.1,
                      help='Weight coefficient for physics constraint term (default: 0.1)')
    parser.add_argument('--dropout_rate', type=float, default=0.1,
                      help='Dropout rate for readout/MLP/KAN layers (set 0.0 to disable dropout)')
    
    # è¯¾ç¨‹å­¦ä¹ å‚æ•°
    parser.add_argument('--curriculum', action='store_true',
                      help='Enable curriculum learning for physics constraint (lambda ramp-up)')
    parser.add_argument('--warmup_epochs', type=int, default=5,
                      help='Number of warmup epochs (lambda=0) for curriculum learning (default: 5)')
    parser.add_argument('--ramp_epochs', type=int, default=10,
                      help='Number of ramp-up epochs for curriculum learning (default: 10)')
    parser.add_argument('--max_lambda', type=float, default=1.0,
                      help='Maximum lambda value for curriculum learning (default: 1.0)')
    
    # ç”¨äºcosineå’Œpolynomialè°ƒåº¦çš„steps_per_epochä¼°è®¡
    parser.add_argument('--steps_per_epoch', type=int, default=100,
                      help='Estimated steps per epoch for cosine/polynomial schedules')
    parser.add_argument('--seed', type=int, default=137,
                      help='Global random seed for reproducibility (default: 42)')
    
    args = parser.parse_args()
    
    if not os.path.exists(args.model_dir):
        os.makedirs(args.model_dir)
        
    main(args)
