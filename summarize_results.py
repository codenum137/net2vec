#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å®éªŒç»“æœæ±‡æ€»åˆ†æå™¨
æ±‡æ€»å¤šç»„å®éªŒçš„ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
"""

import os
import json
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
import argparse
import yaml
import re

class ResultSummarizer:
    """ç»“æœæ±‡æ€»å™¨"""
    
    def __init__(self, config_file="experiment_config.yaml", results_dir="experiment_results"):
        self.config_file = config_file
        self.results_dir = results_dir
        self.config = self.load_config()
        self.results = {}
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(self.config_file, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    def collect_results(self):
        """æ”¶é›†æ‰€æœ‰å®éªŒç»“æœ"""
        print("ğŸ“Š æ­£åœ¨æ”¶é›†å®éªŒç»“æœ...")
        
        for model_name in self.config['models'].keys():
            model_dir = os.path.join(self.results_dir, model_name)
            if not os.path.exists(model_dir):
                continue
                
            self.results[model_name] = {}
            
            # æ”¶é›†å„ç§å®éªŒç»“æœ
            for exp_type in ['evaluate', 'gradient', 'numerical']:
                exp_dir = os.path.join(model_dir, exp_type)
                if os.path.exists(exp_dir):
                    self.results[model_name][exp_type] = self.parse_experiment_results(exp_dir, exp_type)
        
        print(f"âœ… æ”¶é›†åˆ° {len(self.results)} ä¸ªæ¨¡å‹çš„ç»“æœ")
    
    def parse_experiment_results(self, exp_dir, exp_type):
        """è§£æå•ä¸ªå®éªŒç»“æœ"""
        result = {}
        
        try:
            # è¯»å–å®éªŒæ—¥å¿—
            log_file = os.path.join(exp_dir, "experiment_log.json")
            if os.path.exists(log_file):
                with open(log_file, 'r', encoding='utf-8') as f:
                    log_data = json.load(f)
                    result['duration'] = log_data.get('duration', 0)
                    result['success'] = log_data.get('return_code', -1) == 0
            
            # æ ¹æ®å®éªŒç±»å‹è§£æç‰¹å®šç»“æœ
            if exp_type == "evaluate":
                result.update(self.parse_evaluate_results(exp_dir))
            elif exp_type == "gradient":
                result.update(self.parse_gradient_results(exp_dir))
            elif exp_type == "numerical":
                result.update(self.parse_numerical_results(exp_dir))
                
        except Exception as e:
            print(f"âš ï¸  è§£æç»“æœå¤±è´¥ {exp_dir}: {e}")
            result['error'] = str(e)
        
        return result
    
    def parse_evaluate_results(self, exp_dir):
        """è§£æè¯„ä¼°ç»“æœ"""
        result = {}
        
        # æŸ¥æ‰¾è¯„ä¼°ç»“æœæ–‡ä»¶
        for file in os.listdir(exp_dir):
            if file.endswith('.png'):
                result['plot_file'] = os.path.join(exp_dir, file)
        
        # å°è¯•ä»æ—¥å¿—ä¸­æå–æ•°å€¼ç»“æœ
        log_file = os.path.join(exp_dir, "experiment_log.json")
        if os.path.exists(log_file):
            with open(log_file, 'r', encoding='utf-8') as f:
                log_data = json.load(f)
                stdout = log_data.get('stdout', '')
                
                # æå–NSFNetå’ŒGBNçš„æ€§èƒ½æŒ‡æ ‡
                nsfnet_stats = self.extract_performance_stats(stdout, "NSFNet")
                gbn_stats = self.extract_performance_stats(stdout, "GBN")
                
                result['nsfnet'] = nsfnet_stats
                result['gbn'] = gbn_stats
        
        return result
    
    def parse_gradient_results(self, exp_dir):
        """è§£ææ¢¯åº¦éªŒè¯ç»“æœ"""
        result = {}
        
        # æŸ¥æ‰¾ç»“æœæ–‡ä»¶
        for file in os.listdir(exp_dir):
            if file.startswith('sanity_check_results_'):
                result_file = os.path.join(exp_dir, file)
                try:
                    with open(result_file, 'r', encoding='utf-8') as f:
                        content = f.read()
                        
                        # æå–ç‰©ç†ç›´è§‰å¾—åˆ†
                        score_match = re.search(r'Overall Physical Intuition Score: ([\d.]+)%', content)
                        if score_match:
                            result['physics_score'] = float(score_match.group(1)) / 100
                        
                        # æå–å„é¡¹éªŒè¯ç»“æœ
                        result['self_gradient_pass'] = 'Pass' in content and 'Self-influence Gradient > 0: Pass' in content
                        result['cross_gradient_pass'] = 'Cross-influence Gradient > 0: Pass' in content
                        result['monotonic_pass'] = 'Delay Monotonic Increase: Pass' in content
                        result['congestion_pass'] = 'Congestion Sensitivity: Pass' in content
                        
                except Exception as e:
                    print(f"âš ï¸  è§£ææ¢¯åº¦ç»“æœå¤±è´¥: {e}")
        
        return result
    
    def parse_numerical_results(self, exp_dir):
        """è§£ææ•°å€¼åˆ†æç»“æœ"""
        result = {}
        
        # æŸ¥æ‰¾CSVç»“æœæ–‡ä»¶
        csv_file = os.path.join(exp_dir, "detailed_metrics.csv")
        if os.path.exists(csv_file):
            try:
                df = pd.read_csv(csv_file)
                
                # æå–å…³é”®æŒ‡æ ‡
                if 'MAE' in df.columns:
                    result['mae_nsfnet_delay'] = df[df['Dataset'] == 'NSFNet'][df['Metric'] == 'Delay']['MAE'].iloc[0]
                    result['mae_gbn_delay'] = df[df['Dataset'] == 'GBN'][df['Metric'] == 'Delay']['MAE'].iloc[0]
                    result['rmse_nsfnet_delay'] = df[df['Dataset'] == 'NSFNet'][df['Metric'] == 'Delay']['RMSE'].iloc[0]
                    result['rmse_gbn_delay'] = df[df['Dataset'] == 'GBN'][df['Metric'] == 'Delay']['RMSE'].iloc[0]
                    result['mape_nsfnet_delay'] = df[df['Dataset'] == 'NSFNet'][df['Metric'] == 'Delay']['MAPE'].iloc[0]
                    result['mape_gbn_delay'] = df[df['Dataset'] == 'GBN'][df['Metric'] == 'Delay']['MAPE'].iloc[0]
                    
            except Exception as e:
                print(f"âš ï¸  è§£ææ•°å€¼ç»“æœå¤±è´¥: {e}")
        
        return result
    
    def extract_performance_stats(self, text, dataset_name):
        """ä»æ–‡æœ¬ä¸­æå–æ€§èƒ½ç»Ÿè®¡"""
        stats = {}
        
        # æŸ¥æ‰¾å¯¹åº”æ•°æ®é›†çš„éƒ¨åˆ†
        pattern = f"{dataset_name}.*?:" + r"(.*?)(?=\n\n|\n[A-Z]|$)"
        match = re.search(pattern, text, re.DOTALL)
        
        if match:
            section = match.group(1)
            
            # æå–å»¶è¿ŸæŒ‡æ ‡
            delay_pattern = r"DELAY:.*?Mean Abs Error: ([\d.]+).*?Median Abs Error: ([\d.]+).*?P90 Abs Error: ([\d.]+).*?P95 Abs Error: ([\d.]+)"
            delay_match = re.search(delay_pattern, section, re.DOTALL)
            if delay_match:
                stats['delay_mae'] = float(delay_match.group(1))
                stats['delay_median'] = float(delay_match.group(2))
                stats['delay_p90'] = float(delay_match.group(3))
                stats['delay_p95'] = float(delay_match.group(4))
            
            # æå–æŠ–åŠ¨æŒ‡æ ‡
            jitter_pattern = r"JITTER:.*?Mean Abs Error: ([\d.]+).*?Median Abs Error: ([\d.]+).*?P90 Abs Error: ([\d.]+).*?P95 Abs Error: ([\d.]+)"
            jitter_match = re.search(jitter_pattern, section, re.DOTALL)
            if jitter_match:
                stats['jitter_mae'] = float(jitter_match.group(1))
                stats['jitter_median'] = float(jitter_match.group(2))
                stats['jitter_p90'] = float(jitter_match.group(3))
                stats['jitter_p95'] = float(jitter_match.group(4))
        
        return stats
    
    def create_comparison_table(self):
        """åˆ›å»ºå¯¹æ¯”è¡¨æ ¼"""
        print("ğŸ“‹ æ­£åœ¨ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼...")
        
        # å‡†å¤‡æ•°æ®
        comparison_data = []
        
        for model_name, model_results in self.results.items():
            model_config = self.config['models'][model_name]
            
            row = {
                'Model': model_name,
                'Type': model_config['model_type'].upper(),
                'Physics': model_config['physics_type'].title(),
                'Lambda': model_config['lambda_physics'],
            }
            
            # æ·»åŠ è¯„ä¼°ç»“æœ
            if 'evaluate' in model_results:
                eval_data = model_results['evaluate']
                if 'nsfnet' in eval_data:
                    row.update({
                        'NSFNet_Delay_MAE': eval_data['nsfnet'].get('delay_mae', np.nan),
                        'NSFNet_Delay_P95': eval_data['nsfnet'].get('delay_p95', np.nan),
                        'NSFNet_Jitter_MAE': eval_data['nsfnet'].get('jitter_mae', np.nan),
                    })
                if 'gbn' in eval_data:
                    row.update({
                        'GBN_Delay_MAE': eval_data['gbn'].get('delay_mae', np.nan),
                        'GBN_Delay_P95': eval_data['gbn'].get('delay_p95', np.nan),
                        'GBN_Jitter_MAE': eval_data['gbn'].get('jitter_mae', np.nan),
                    })
            
            # æ·»åŠ æ¢¯åº¦éªŒè¯ç»“æœ
            if 'gradient' in model_results:
                grad_data = model_results['gradient']
                row.update({
                    'Physics_Score': grad_data.get('physics_score', np.nan),
                    'Self_Gradient_Pass': grad_data.get('self_gradient_pass', False),
                    'Cross_Gradient_Pass': grad_data.get('cross_gradient_pass', False),
                    'Monotonic_Pass': grad_data.get('monotonic_pass', False),
                })
            
            # æ·»åŠ æ•°å€¼åˆ†æç»“æœ
            if 'numerical' in model_results:
                num_data = model_results['numerical']
                row.update({
                    'NSFNet_MAE': num_data.get('mae_nsfnet_delay', np.nan),
                    'GBN_MAE': num_data.get('mae_gbn_delay', np.nan),
                    'NSFNet_RMSE': num_data.get('rmse_nsfnet_delay', np.nan),
                    'GBN_RMSE': num_data.get('rmse_gbn_delay', np.nan),
                })
            
            comparison_data.append(row)
        
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(comparison_data)
        
        # ä¿å­˜ä¸ºCSV
        summary_dir = os.path.join(self.results_dir, "summary")
        os.makedirs(summary_dir, exist_ok=True)
        csv_file = os.path.join(summary_dir, "model_comparison.csv")
        df.to_csv(csv_file, index=False)
        
        print(f"âœ… å¯¹æ¯”è¡¨æ ¼ä¿å­˜åœ¨: {csv_file}")
        
        # æ˜¾ç¤ºæœ€ä½³æ¨¡å‹
        self.find_best_models(df, summary_dir)
        
        return df
    
    def find_best_models(self, df, summary_dir):
        """æ‰¾å‡ºæœ€ä½³æ¨¡å‹"""
        print("\nğŸ† å¯»æ‰¾æœ€ä½³æ¨¡å‹...")
        
        best_models = {}
        
        # æŒ‰ä¸åŒæŒ‡æ ‡æ‰¾æœ€ä½³æ¨¡å‹
        metrics = [
            ('NSFNet_Delay_MAE', 'æœ€ä½NSFNetå»¶è¿ŸMAE'),
            ('GBN_Delay_MAE', 'æœ€ä½GBNå»¶è¿ŸMAE'),
            ('Physics_Score', 'æœ€é«˜ç‰©ç†ç›´è§‰å¾—åˆ†'),
            ('NSFNet_MAE', 'æœ€ä½NSFNetæ•°å€¼MAE'),
            ('GBN_MAE', 'æœ€ä½GBNæ•°å€¼MAE')
        ]
        
        best_summary = []
        
        for metric, description in metrics:
            if metric in df.columns:
                if 'MAE' in metric or 'RMSE' in metric:
                    # è¶Šå°è¶Šå¥½
                    best_idx = df[metric].idxmin()
                    best_value = df.loc[best_idx, metric]
                    best_model = df.loc[best_idx, 'Model']
                else:
                    # è¶Šå¤§è¶Šå¥½
                    best_idx = df[metric].idxmax()
                    best_value = df.loc[best_idx, metric]
                    best_model = df.loc[best_idx, 'Model']
                
                best_models[metric] = (best_model, best_value)
                best_summary.append(f"{description}: {best_model} ({best_value:.4f})")
                print(f"   {description}: {best_model} ({best_value:.4f})")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹æ€»ç»“
        best_file = os.path.join(summary_dir, "best_models.txt")
        with open(best_file, 'w', encoding='utf-8') as f:
            f.write("æœ€ä½³æ¨¡å‹æ€»ç»“\n")
            f.write("=" * 50 + "\n\n")
            for summary in best_summary:
                f.write(summary + "\n")
        
        print(f"âœ… æœ€ä½³æ¨¡å‹æ€»ç»“ä¿å­˜åœ¨: {best_file}")
    
    def create_visualizations(self):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        print("ğŸ“Š æ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
        
        summary_dir = os.path.join(self.results_dir, "summary")
        os.makedirs(summary_dir, exist_ok=True)
        
        # åˆ›å»ºæ€§èƒ½å¯¹æ¯”å›¾
        self.plot_performance_comparison(summary_dir)
        
        # åˆ›å»ºç‰©ç†çº¦æŸæ•ˆæœå›¾
        self.plot_physics_effect(summary_dir)
        
        print("âœ… å¯è§†åŒ–å›¾è¡¨ç”Ÿæˆå®Œæˆ")
    
    def plot_performance_comparison(self, output_dir):
        """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
        # å‡†å¤‡æ•°æ®
        data_for_plot = []
        
        for model_name, model_results in self.results.items():
            model_config = self.config['models'][model_name]
            
            if 'evaluate' in model_results:
                eval_data = model_results['evaluate']
                
                # NSFNetæ•°æ®
                if 'nsfnet' in eval_data:
                    data_for_plot.append({
                        'Model': model_name,
                        'Type': model_config['model_type'].upper(),
                        'Physics': model_config['physics_type'],
                        'Lambda': model_config['lambda_physics'],
                        'Dataset': 'NSFNet',
                        'Delay_MAE': eval_data['nsfnet'].get('delay_mae', np.nan),
                        'Jitter_MAE': eval_data['nsfnet'].get('jitter_mae', np.nan)
                    })
                
                # GBNæ•°æ®
                if 'gbn' in eval_data:
                    data_for_plot.append({
                        'Model': model_name,
                        'Type': model_config['model_type'].upper(),
                        'Physics': model_config['physics_type'],
                        'Lambda': model_config['lambda_physics'],
                        'Dataset': 'GBN',
                        'Delay_MAE': eval_data['gbn'].get('delay_mae', np.nan),
                        'Jitter_MAE': eval_data['gbn'].get('jitter_mae', np.nan)
                    })
        
        if not data_for_plot:
            print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾")
            return
        
        df_plot = pd.DataFrame(data_for_plot)
        
        # åˆ›å»ºå­å›¾
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # å»¶è¿ŸMAEå¯¹æ¯”
        sns.boxplot(data=df_plot, x='Type', y='Delay_MAE', hue='Physics', ax=axes[0,0])
        axes[0,0].set_title('Delay MAE Comparison')
        axes[0,0].set_ylabel('Mean Absolute Error')
        
        # æŠ–åŠ¨MAEå¯¹æ¯”
        sns.boxplot(data=df_plot, x='Type', y='Jitter_MAE', hue='Physics', ax=axes[0,1])
        axes[0,1].set_title('Jitter MAE Comparison')
        axes[0,1].set_ylabel('Mean Absolute Error')
        
        # Lambdaå‚æ•°æ•ˆæœ
        sns.scatterplot(data=df_plot, x='Lambda', y='Delay_MAE', hue='Type', style='Physics', ax=axes[1,0])
        axes[1,0].set_title('Lambda Effect on Delay MAE')
        axes[1,0].set_xlabel('Lambda Parameter')
        axes[1,0].set_ylabel('Delay MAE')
        
        # æ•°æ®é›†é—´æ€§èƒ½å¯¹æ¯”
        sns.boxplot(data=df_plot, x='Dataset', y='Delay_MAE', hue='Type', ax=axes[1,1])
        axes[1,1].set_title('Cross-topology Performance')
        axes[1,1].set_ylabel('Delay MAE')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_physics_effect(self, output_dir):
        """ç»˜åˆ¶ç‰©ç†çº¦æŸæ•ˆæœå›¾"""
        # å‡†å¤‡ç‰©ç†çº¦æŸæ•°æ®
        physics_data = []
        
        for model_name, model_results in self.results.items():
            model_config = self.config['models'][model_name]
            
            if 'gradient' in model_results:
                grad_data = model_results['gradient']
                
                physics_data.append({
                    'Model': model_name,
                    'Type': model_config['model_type'].upper(),
                    'Physics': model_config['physics_type'],
                    'Lambda': model_config['lambda_physics'],
                    'Physics_Score': grad_data.get('physics_score', np.nan)
                })
        
        if not physics_data:
            print("âš ï¸  æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®ç”Ÿæˆç‰©ç†çº¦æŸæ•ˆæœå›¾")
            return
        
        df_physics = pd.DataFrame(physics_data)
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # ç‰©ç†ç›´è§‰å¾—åˆ†å¯¹æ¯”
        sns.boxplot(data=df_physics, x='Type', y='Physics_Score', hue='Physics', ax=axes[0])
        axes[0].set_title('Physics Intuition Score Comparison')
        axes[0].set_ylabel('Physics Score')
        
        # Lambdaå‚æ•°å¯¹ç‰©ç†å¾—åˆ†çš„å½±å“
        sns.scatterplot(data=df_physics, x='Lambda', y='Physics_Score', hue='Type', style='Physics', ax=axes[1])
        axes[1].set_title('Lambda Parameter Effect on Physics Score')
        axes[1].set_xlabel('Lambda Parameter')
        axes[1].set_ylabel('Physics Score')
        
        plt.tight_layout()
        plt.savefig(os.path.join(output_dir, 'physics_effect.png'), dpi=300, bbox_inches='tight')
        plt.close()
    
    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        print("\nğŸ“Š æ­£åœ¨ç”Ÿæˆå®Œæ•´å®éªŒæŠ¥å‘Š...")
        
        # æ”¶é›†ç»“æœ
        self.collect_results()
        
        if not self.results:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
            return
        
        # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
        df = self.create_comparison_table()
        
        # åˆ›å»ºå¯è§†åŒ–å›¾è¡¨
        self.create_visualizations()
        
        print(f"\nâœ… å®éªŒæŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.results_dir}/summary/")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®éªŒç»“æœæ±‡æ€»åˆ†æå™¨')
    parser.add_argument('--config', default='experiment_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--results_dir', default='experiment_results', help='ç»“æœç›®å½•')
    
    args = parser.parse_args()
    
    try:
        summarizer = ResultSummarizer(args.config, args.results_dir)
        summarizer.generate_report()
        
    except Exception as e:
        print(f"ğŸ’¥ æ±‡æ€»å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0

if __name__ == '__main__':
    import sys
    sys.exit(main())
