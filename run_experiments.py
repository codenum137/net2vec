#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–å®éªŒè¿è¡Œå™¨
åŸºäºé…ç½®æ–‡ä»¶è¿è¡Œå¤šç»„RouteNetå®éªŒ
"""

import yaml
import os
import sys
import subprocess
import argparse
import time
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from pathlib import Path
import json

class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, config_file="experiment_config.yaml"):
        """åˆå§‹åŒ–å®éªŒè¿è¡Œå™¨"""
        self.config_file = config_file
        self.config = self.load_config()
        self.results = {}
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not os.path.exists(self.config_file):
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ {self.config_file} ä¸å­˜åœ¨")
        
        with open(self.config_file, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        
        print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {self.config_file}")
        print(f"ğŸ“‹ å‘ç° {len(config['models'])} ä¸ªæ¨¡å‹é…ç½®")
        print(f"ğŸ§ª å‘ç° {len(config['experiments'])} ç§å®éªŒç±»å‹")
        
        return config
    
    def validate_model_paths(self):
        """éªŒè¯æ¨¡å‹è·¯å¾„æ˜¯å¦å­˜åœ¨"""
        missing_models = []
        
        for model_name, model_config in self.config['models'].items():
            model_dir = model_config['delay_model_dir']
            if not os.path.exists(model_dir):
                missing_models.append(f"{model_name}: {model_dir}")
        
        if missing_models:
            print("âš ï¸  ä»¥ä¸‹æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨:")
            for missing in missing_models:
                print(f"   - {missing}")
            return False
        
        print("âœ… æ‰€æœ‰æ¨¡å‹è·¯å¾„éªŒè¯é€šè¿‡")
        return True
    
    def build_command(self, experiment_type, model_name, model_config):
        """æ„å»ºå®éªŒå‘½ä»¤"""
        exp_config = self.config['experiments'][experiment_type]
        global_settings = self.config['global_settings']
        
        # åŸºç¡€å‘½ä»¤
        script_path = exp_config['script']
        cmd = ["python", script_path]
        
        # è¾“å‡ºç›®å½•
        output_dir = os.path.join(
            global_settings['base_output_dir'],
            model_name,
            experiment_type
        )
        
        # æ„å»ºå‚æ•°
        if experiment_type == "evaluate":
            cmd.extend([
                "--delay_model_dir", model_config['delay_model_dir'],
                "--nsfnet_test_dir", global_settings['nsfnet_test_dir'],
                "--gbn_test_dir", global_settings['gbn_test_dir'],
                "--output_dir", output_dir,
                "--batch_size", str(global_settings['batch_size']),
                "--num_samples", str(global_settings['num_samples'])
            ])
            if model_config['use_kan']:
                cmd.append("--kan")
                
        elif experiment_type == "gradient":
            cmd.extend([
                "--model_dir", model_config['delay_model_dir'],
                "--output_dir", output_dir,
                "--target", "delay",
                "--traffic_min", str(global_settings['traffic_min']),
                "--traffic_max", str(global_settings['traffic_max']),
                "--num_points", str(global_settings['num_points'])
            ])
            if model_config['use_kan']:
                cmd.append("--use_kan")
                
        elif experiment_type == "numerical":
            cmd.extend([
                "--model_dir", model_config['delay_model_dir'],
                "--nsfnet_test_dir", global_settings['nsfnet_test_dir'],
                "--gbn_test_dir", global_settings['gbn_test_dir'],
                "--output_dir", output_dir,
                "--batch_size", str(global_settings['batch_size']),
                "--num_samples", str(global_settings['num_samples'])
            ])
            if model_config['use_kan']:
                cmd.append("--kan")
        
        return cmd, output_dir
    
    def run_single_experiment(self, experiment_type, model_name, model_config):
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        try:
            cmd, output_dir = self.build_command(experiment_type, model_name, model_config)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            os.makedirs(output_dir, exist_ok=True)
            
            # è®°å½•å¼€å§‹æ—¶é—´
            start_time = time.time()
            
            print(f"ğŸš€ å¼€å§‹å®éªŒ: {model_name} - {experiment_type}")
            print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_dir}")
            print(f"âš¡ å‘½ä»¤: {' '.join(cmd)}")
            
            # è¿è¡Œå®éªŒ
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                cwd=os.getcwd()
            )
            
            # è®¡ç®—è¿è¡Œæ—¶é—´
            duration = time.time() - start_time
            
            # ä¿å­˜ç»“æœ
            experiment_result = {
                'model_name': model_name,
                'experiment_type': experiment_type,
                'command': ' '.join(cmd),
                'output_dir': output_dir,
                'duration': duration,
                'return_code': result.returncode,
                'stdout_lines': result.stdout.split('\n') if result.stdout else [],
                'stderr_lines': result.stderr.split('\n') if result.stderr else [],
                'timestamp': datetime.now().isoformat()
            }
            
            if result.returncode == 0:
                print(f"âœ… å®éªŒæˆåŠŸ: {model_name} - {experiment_type} (è€—æ—¶: {duration:.1f}s)")
            else:
                print(f"âŒ å®éªŒå¤±è´¥: {model_name} - {experiment_type}")
                print(f"ğŸ’¬ é”™è¯¯ä¿¡æ¯: {result.stderr}")
                
            # ä¿å­˜å®éªŒæ—¥å¿—
            log_file = os.path.join(output_dir, "experiment_log.json")
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(experiment_result, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ä¸€ä¸ªæ›´æ˜“è¯»çš„æ–‡æœ¬æ—¥å¿—
            readable_log_file = os.path.join(output_dir, "experiment_log.txt")
            with open(readable_log_file, 'w', encoding='utf-8') as f:
                f.write(f"å®éªŒ: {model_name} - {experiment_type}\n")
                f.write(f"{'='*60}\n")
                f.write(f"å¼€å§‹æ—¶é—´: {experiment_result['timestamp']}\n")
                f.write(f"è¿è¡Œæ—¶é—´: {duration:.2f} ç§’\n")
                f.write(f"è¿”å›ç : {result.returncode}\n")
                f.write(f"å‘½ä»¤: {' '.join(cmd)}\n\n")
                
                f.write("æ ‡å‡†è¾“å‡º:\n")
                f.write("-" * 40 + "\n")
                f.write(result.stdout if result.stdout else "æ— è¾“å‡º\n")
                f.write("\n")
                
                if result.stderr:
                    f.write("é”™è¯¯è¾“å‡º:\n")
                    f.write("-" * 40 + "\n")
                    f.write(result.stderr)
                    f.write("\n")
            
            return experiment_result
            
        except Exception as e:
            error_result = {
                'model_name': model_name,
                'experiment_type': experiment_type,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }
            print(f"ğŸ’¥ å®éªŒå¼‚å¸¸: {model_name} - {experiment_type}: {e}")
            return error_result
    
    def run_experiments(self, selected_models=None, selected_experiments=None, parallel=False, max_workers=4):
        """è¿è¡Œå®éªŒ"""
        # ç­›é€‰æ¨¡å‹
        if selected_models:
            models_to_run = {k: v for k, v in self.config['models'].items() if k in selected_models}
        else:
            models_to_run = self.config['models']
        
        # ç­›é€‰å®éªŒç±»å‹
        if selected_experiments:
            experiments_to_run = [exp for exp in selected_experiments if exp in self.config['experiments']]
        else:
            experiments_to_run = list(self.config['experiments'].keys())
        
        print(f"\nğŸ¯ å‡†å¤‡è¿è¡Œå®éªŒ:")
        print(f"ğŸ“Š æ¨¡å‹æ•°é‡: {len(models_to_run)}")
        print(f"ğŸ§ª å®éªŒç±»å‹: {experiments_to_run}")
        print(f"ğŸ”¢ æ€»å®éªŒæ•°: {len(models_to_run) * len(experiments_to_run)}")
        print(f"âš¡ å¹¶è¡Œæ‰§è¡Œ: {'æ˜¯' if parallel else 'å¦'}")
        
        # ç¡®è®¤ç»§ç»­
        response = input("\nç¡®è®¤å¼€å§‹å®éªŒ? (y/N): ")
        if response.lower() != 'y':
            print("å–æ¶ˆå®éªŒ")
            return
        
        # è®°å½•æ‰€æœ‰å®éªŒä»»åŠ¡
        all_tasks = []
        for model_name, model_config in models_to_run.items():
            for experiment_type in experiments_to_run:
                all_tasks.append((experiment_type, model_name, model_config))
        
        # è¿è¡Œå®éªŒ
        start_time = time.time()
        
        if parallel and len(all_tasks) > 1:
            # å¹¶è¡Œè¿è¡Œ
            print(f"\nğŸ”„ ä½¿ç”¨ {max_workers} ä¸ªå¹¶è¡Œå·¥ä½œè¿›ç¨‹")
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                future_to_task = {
                    executor.submit(self.run_single_experiment, exp_type, model_name, model_config): 
                    (exp_type, model_name) 
                    for exp_type, model_name, model_config in all_tasks
                }
                
                for future in as_completed(future_to_task):
                    exp_type, model_name = future_to_task[future]
                    try:
                        result = future.result()
                        self.results[f"{model_name}_{exp_type}"] = result
                    except Exception as e:
                        print(f"ğŸ’¥ ä»»åŠ¡å¼‚å¸¸ {model_name}_{exp_type}: {e}")
        else:
            # ä¸²è¡Œè¿è¡Œ
            print(f"\nğŸ”„ ä¸²è¡Œè¿è¡Œå®éªŒ")
            for i, (exp_type, model_name, model_config) in enumerate(all_tasks, 1):
                print(f"\nğŸ“Š è¿›åº¦: {i}/{len(all_tasks)}")
                result = self.run_single_experiment(exp_type, model_name, model_config)
                self.results[f"{model_name}_{exp_type}"] = result
        
        # æ€»ç»“
        total_time = time.time() - start_time
        successful = sum(1 for r in self.results.values() if r.get('return_code') == 0)
        failed = len(self.results) - successful
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š å®éªŒå®Œæˆæ€»ç»“")
        print(f"{'='*60}")
        print(f"â±ï¸  æ€»è€—æ—¶: {total_time:.1f} ç§’")
        print(f"âœ… æˆåŠŸ: {successful} ä¸ª")
        print(f"âŒ å¤±è´¥: {failed} ä¸ª")
        print(f"ğŸ“ ç»“æœä¿å­˜åœ¨: {self.config['global_settings']['base_output_dir']}")
        
        # ä¿å­˜å®éªŒæ€»ç»“
        summary_dir = os.path.join(self.config['global_settings']['base_output_dir'], "summary")
        os.makedirs(summary_dir, exist_ok=True)
        
        summary_file = os.path.join(summary_dir, f"experiment_summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump({
                'total_experiments': len(all_tasks),
                'successful': successful,
                'failed': failed,
                'total_time': total_time,
                'results': self.results,
                'config': self.config_file
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ è¯¦ç»†æ€»ç»“ä¿å­˜åœ¨: {summary_file}")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–RouteNetå®éªŒè¿è¡Œå™¨')
    parser.add_argument('--config', default='experiment_config.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--models', nargs='+', help='æŒ‡å®šè¦è¿è¡Œçš„æ¨¡å‹ (é»˜è®¤å…¨éƒ¨)')
    parser.add_argument('--experiments', nargs='+', 
                       choices=['evaluate', 'gradient', 'numerical'],
                       help='æŒ‡å®šè¦è¿è¡Œçš„å®éªŒç±»å‹ (é»˜è®¤å…¨éƒ¨)')
    parser.add_argument('--parallel', action='store_true', help='å¹¶è¡Œè¿è¡Œå®éªŒ')
    parser.add_argument('--max_workers', type=int, default=4, help='æœ€å¤§å¹¶è¡Œå·¥ä½œè¿›ç¨‹æ•°')
    parser.add_argument('--validate_only', action='store_true', help='ä»…éªŒè¯é…ç½®ï¼Œä¸è¿è¡Œå®éªŒ')
    
    args = parser.parse_args()
    
    try:
        # åˆå§‹åŒ–è¿è¡Œå™¨
        runner = ExperimentRunner(args.config)
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        if not runner.validate_model_paths():
            print("âŒ æ¨¡å‹è·¯å¾„éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶")
            return 1
        
        if args.validate_only:
            print("âœ… é…ç½®éªŒè¯å®Œæˆ")
            return 0
        
        # è¿è¡Œå®éªŒ
        runner.run_experiments(
            selected_models=args.models,
            selected_experiments=args.experiments,
            parallel=args.parallel,
            max_workers=args.max_workers
        )
        
        return 0
        
    except Exception as e:
        print(f"ğŸ’¥ è¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

if __name__ == '__main__':
    sys.exit(main())
